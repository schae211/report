---
title: "Conclusions"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: no
    toc_float: no
bibliography: references.bib
csl: science.csl
---

# Discussion & Conclusions:

* The proposed framework, to supply the model with which the view-specific models are trained as a function to the `run_misty()` call, seems to work fine. The performance measures (multi R2 and gain R2) are similar for most models which were included. Only, the linear support vector machine (SVM), multi-layer perceptron (MLP), and linear gradient boosting (LBOOST) seem to stand out as being worse. The performance of the linear model (LM) highly depends on the dataset. Sometimes LM performs as well as random forest (RF) and sometimes it is worse. Tree gradient boosting (TBOOST) performs as well as RF on all datasets. Multivariate adaptive regression splines are almost as good as the RF model, and the same holds true for the bagged MARS model.

* Looking at the heatmaps of the cosine similarities of the importance vectors, one can see the same trend in almost all data sets. The results of RF, TBOOST, and MARS cluster together on one side and LM, LBOOST, SVM on the other side. Another observed trend was the decline of the overall cosine similarity between the different algorithms when going from the intra- to the juxta- to the paraview.

* The runtimes for the `synthetic` datasets with one paraview (`l=12`) show that LM is the fastest model followed by MARS. In the middle there are TBOOST and RF which are equally fast. The slowest models are BGMARS, SVM and MLP (which is by a large margin the slowest model). However, when looking at these time we should not forgest that they do not provide information about scalability. For example the MARS model with the default parameters (`degree = 2, fast.k = 5, thresh=0.01`) is faster than RF on the synthetic dataset which has 10 samples, with approximately 4000 cells and 11 markers in each sample, but MARS is much slower on datasets with more markers as described in the next paragraph.

* MARS (multivariate adaptive regression splines) models are not as scalable as random forests, gradient boosting models, and linear models. Especially when the number of predictors ("markers") increases and the number of possible interaction terms (`degree` hyperparameter) is larger than 1, the runtimes of MARS increases drastically, because in the model building process an increasing number of combinations has to be tested. Subsampling the data only helps a little bit here. More importantly the number of features must be reduced which can be achieved for example by only considering spatially variable markers (e.g. computed with SPARK-X [@10.1186/s13059-021-02404-0]). Furthermore, the hyperparameters can be optimized for decreasing the runtimes, for example by decreasing `fast.k` or increasing `thresh` as described in [Notes on the earth package](http://www.milbo.org/doc/earth-notes.pdf) by Stephen Milborrow (see 2.6 Execution time).

* SVM and MLP do not seem to be suited for modeling views within the current framework of MISTy. They are not as scalable as random forest, gradient boosting models, or linear models. Furthermore Multi-layer Perceptrons are very sensitive to hyperparameters and thus would need hyperparameter optimization for each view for each marker which is not feasible. The scalability of MISTy is an important feature that allows to quickly explore different parameters for computing views. Data analysis is an iterative process and the quicker one can explore data, the better.

* Linear Models are sometimes as good as non-linear models such as random forest or gradient boosting. Thereby, their interpretability and their runtime is better. It would be benefical to understand what sample characteristics determine whether linear models are suitable or not.

* See in `test-misty.R` (commented lines at 469-470 and 493-494), SVM and MLP and not fully reproducible yet. The results of different runs are similar but not exactly the same as the haststrings differ.

# Open Questions & Future Directions:

* How can we focus the MISTy analysis on certain parts of the tissue slides? 
(for example the tumor immune border). And could we incorporate some kind
of comparitive MISTy were we run MISTy "automatically" in two different
regions on the same slide and compare the results we obtained in a meaningful
way (e.g. which interactions are present here but not there.)

* 

# References

