---
title: "Conclusions"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: no
    toc_float: no
bibliography: references.bib
csl: science.csl
---

# Discussion & Conclusions:

* The proposed framework, to supply the model with which the view-specific models are trained as a function to the `run_misty()` call, seems to work fine. The performance measures (multi R2 and gain R2) are similar for most models which were included. Only, the linear support vector machine (SVM), multi-layer perceptron (MLP), and linear gradient boosting (LBOOST) stand out as being worse. The relative performance of the linear model (LM) highly depends on the dataset. Sometimes LM performs as well as random forest (RF) and sometimes it is worse. Tree gradient boosting (TBOOST) performs as well as RF on all datasets. Multivariate adaptive regression splines are almost as good as the RF model on most datasets, and the same holds true for the bagged MARS model.

* Overall, the performance results for all algorithms underline why the random forest is the default algorithm. It has good performance, is fast, scalable, and not very sensitive to hyperparameter choices (shown in the hyperparameter section of the benchmark).

* Looking at the heatmaps of the cosine similarities of the importance vectors, one can see the same trend in almost all data sets. The results of RF, TBOOST, and MARS cluster together on one side and LM, LBOOST, SVM on the other side. Another observed trend was the decline of the overall cosine similarity between the different algorithms when going from the intra- to the juxta- to the paraview.

* The runtimes for the `synthetic` datasets with one paraview (`l=12`) show that LM is the fastest model followed by MARS. In the middle there are TBOOST and RF which are equally fast. The slowest models are BGMARS, SVM and MLP (which is by a large margin the slowest model). However, when looking at these time one should not forget that they do not provide information about scalability. For example the MARS model with the default parameters in functions.R (`degree = 2, fast.k = 5, thresh=0.01`) is faster than RF on the synthetic dataset which has 10 samples with approximately 4000 cells and 11 markers in each sample, but MARS is much slower on datasets with more markers as described in the next paragraph.

* MARS (multivariate adaptive regression splines) models are not as scalable as random forests, gradient boosting models, and linear models. Especially when the number of predictors ("markers") increases and the number of possible interaction terms (`degree` hyperparameter) is larger than 1, the runtime of MARS increases drastically, because in the model building process an increasing number of combinations has to be tested. Subsampling the data helps a little bit here. More importantly the number of features must be reduced which can be achieved for example by only considering spatially variable markers (e.g. computed with SPARK-X [@10.1186/s13059-021-02404-0]). Furthermore, the hyperparameters can be optimized for decreasing the runtimes, for example by decreasing `fast.k` or increasing `thresh` as described in [Notes on the earth package](http://www.milbo.org/doc/earth-notes.pdf) by Stephen Milborrow (see 2.6 Execution time), but this comes at the cost of reducing the performance.

* SVM and MLP do not seem to be suited for modeling views within the current framework of MISTy. They are not as scalable as random forest, gradient boosting models, or linear models. Furthermore MLPs are very sensitive to hyperparameters choices and thus would need hyperparameter optimization for each view for each marker which is not feasible. The scalability of MISTy is an important feature that allows to quickly explore different parameters for computing views. Data analysis is an iterative process and the quicker one can explore data, the better.

* Linear Models are sometimes as good as non-linear models such as random forest or gradient boosting. Thereby, their interpretability and their runtime is better. It would be benefical to understand what sample characteristics determine whether linear models are suitable or not.

* See in `test-misty.R` (commented lines at 469-470 and 493-494), SVM and MLP and not fully reproducible yet. The results of different runs are similar but not exactly the same as the hashstrings differ.

* Looking at [case study A](https://schae211.github.io/report/case_study_A.html) we see how difficult it is to predict the identity of a cell using the identity of the neighboring cells, especially when the classes are highly unbalanced. It might be worth to test whether one could use SMOTE techniques in such cases within the MISTy framework.

# Open Questions & Future Directions:

* It is somewhat difficult to choose the right parameters for computing juxta- and paraviews, and thus it might be useful if one could interactively explore the weights of the neighboring cells for a given cells while hovering over this cells. Just like [sleepwalker](https://anders-biostat.github.io/sleepwalk/) [@10.1101/gr.251447.119] can be used to explore the distance between cells in dimensional-reduced embeddings (e.g. UMAP).

* Since one can expect spatial omics datasets to get even larger, subsampling and smart sampling techniques should be tested and systematically evaluated. For the `merfish_bc` dataset for example subsampling 60% of the training data in the `mars_model` did not reduce the performance.

* As Jovan proposed, moving in the direction of AutoML, i.e. also learn which parameters work well for which kind of data will be an interesting endeavor.

* It might be necessary to lower the bar to use MISTy, meaning to provide helper functions to prepare spatial omics data to be analyzed with MISTy (e.g. fix names, check for zero variance, ...). Functions to make celltype-based analysis more easily might also be interesting, meaning to compute a paraview considering only the expression in neighboring CD4 T-cells for example. In general we will have to set up some kinds of pipelines to showcase the flexible of MISTy.

# References

