---
title: "Methods"
editor_options: 
  chunk_output_type: inline
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: yes
    toc_float: yes
bibliography: references.bib
csl: science.csl
link-citations: yes
---

Setup.

```{r setup}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
knitr::opts_knit$set(root.dir = "~/Saez/report")
```

Loaded packages.

```{r}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
suppressPackageStartupMessages(library(tidyverse))
plan("multisession", workers=14)
```

In this file, I will briefly explain how I envision the new MISTy workflow. I refactored and expanded the code such that users can supply their own function to model each view if they would like. If not, we ship MISTy with several implemented functions such as gradient boosting that can simply be plugged into the model. More details can be found below.

# Overivew

## Data

Here we will be using the synthetic dataset
supplied in the MISTy package. As explained in the 
[Get Started Vignette](https://saezlab.github.io/mistyR/articles/mistyR.html),
this dataset is based on a two-dimensional cellular automata model which 
models the production, diffusion, degradation and interactions of 11 
molecular species in 4 different cell types. 

In total there are 10 samples (tibbles), each of which contains about 4000 cells and 
their corresponding expresssion, and position in a 100x100 grid
(randomly assigned) as well as the cell type identity. (For more information
see `help("synthetic")`).

```{r}
data("synthetic")
```

## MISTy Views

For the sake of keeping things simply, we will only look at the first sample of the synthetic dataset.

We will start by creating two MISTy views: a) Intraview and b) Paraview
with a Gaussian kernel (default) a radius of 10 and no zone of indifference.

```{r}
expr <- synthetic$synthetic1 %>% dplyr::select(-c(row, col, type))
pos <- synthetic$synthetic1 %>% dplyr::select(c(row, col))
misty.views <- expr %>%
  create_initial_view() %>%
  add_paraview(l = 10, positions = pos)
```

## Running MISty

The default ML algorithm is still random forest and thus we do not even
have to specify it when we want to use it.

```{r}
misty.run <- misty.views %>%
  run_misty()
```

To make things more explicitly the above call is the same as:

```{r}
misty.run <- misty.views %>%
  run_misty(model.function = random_forest_model)
```

But what is this `random_forest_model` actually?

```{r}
random_forest_model
```

It is a function that takes 3 arguments: The data of a single view (`view_data`), a `target`, and a random `seed` to make things reproducible. Let's assume that we are looking at the `paraview`.

```{r}
misty.views$paraview.10$data %>% slice_head(n=6)
```

Now, the `random_forest_model` predict the expression of the target variable using a random forest (implemented by `ranger`). Importantly, within the `run_misty` function before the `view_data` are supplied to the random_forest_model, the target column is replaced by the actual values from the `intraview` as shown below with the target being "ECM".

```{r}
target <- "ECM"
expr <- misty.views$intraview$data
target.vector <- expr %>% dplyr::pull(target)
view_data <- misty.views$paraview.10$data %>%
  mutate(!!target := target.vector)
# compare to the above
view_data %>% slice_head(n=6)
```

And then the RF algorithm is called with the default algorithms merged
with additional ellipsis arguments from the `run_misty()` call.

```{r}
seed <- 42 # default
ellipsis.args <- list() # assume no ellipsis arguments were given

# default ranger arguments
algo.arguments <- list(
  formula = stats::as.formula(paste0(target, " ~ .")),
  data = view_data,
  num.trees = 100,
  importance = "impurity",
  mtry = NULL, 
  verbose = FALSE, 
  num.threads = 1,
  seed = seed)

if (!(length(ellipsis.args) == 0)) {
  algo.arguments <- merge_2(algo.arguments, ellipsis.args)
}

model <- do.call(ranger::ranger, algo.arguments)

predictions <- tibble::tibble(index = seq_len(nrow(view_data)), 
                              prediction = model$predictions)

list(unbiased.predictions = predictions, 
     importances = model$variable.importance) %>%
  str() # add str here to show output
```

And the `random_forest_model` returns a list of unbiased predictions (which are here out-of-bag predictions) and importances (which are there reduction in the RSS averaged across each a split a given predictor is used in).

(Note that `merge_2` is a function that is also exported by `mistyR`)

```{r echo=FALSE}
rm(view_data, target.vector, expr, model, ellipsis.args, algo.arguments)
```

As seen above, we can also easily supply arguments to the `ranger` RF implementation. Say we would like to increase the number of trees and use another splitrule. Let's compare the runtime for example (should be higher with more trees).

```{r}
tictoc::tic()
misty.run <- misty.views %>%
  run_misty(model.function = random_forest_model, num.trees = 500, 
            splitrule = "extratrees")
tictoc::toc()
unlink("results")
```

```{r}
tictoc::tic()
misty.run <- misty.views %>%
  run_misty(model.function = random_forest_model, num.trees = 100, 
            splitrule = "extratrees")
tictoc::toc()
```

## MISTy results

On the side of processing and plotting the results nothing has changed.

```{r}
misty.results <- collect_results(misty.run)
misty.results %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
unlink("results")
```

Below all the supplied function will be described in more detail.

## General Remarks

Useful ressources for an overview of regression models include [@noauthororeditor2013applied, @James2013, @hastie01statisticallearning].

Depending on the machine learning model different approaches were used for performance assessment. For random forest out-of-bag estimates can be used which is computationally much more efficient than k-fold cross-validation. However, it has been shown that the out-of-bag estimates may underestimate the accuracy of the model for unobserved data [@10.1371/journal.pone.0201904]. For algorithms not based on bagging (trained with boostrapped samples) k-fold cross-validation is used to assess the performance. 

Determining the importances of the predictors (e.g. all other markers) for each target is much more algorithm specific than the performance estimation. For random forests for example the importance corresponds to the average decrease in the RSS over all splits for a given predictor. A similar idea is used to asses the importances of MARS models. During the backward pass (when terms are deleted from the model), the decrease of the RSS is measured for each subset relative to the previous subset. Then for each variable, the average decrease in RSS over all subsets in which the variable is included is used as importance. However if the `degree` is larger than one, the importance for a given model term containing several predictors is spread equally for all predictors. (see [Notes on the earth package](http://www.milbo.org/doc/earth-notes.pdf) for more details). Also the importance extraction for boosted trees (in `xgboost`) works similar to the method in random forest. Importantly one should note that random forests and boosted trees deal very different with correlated features since random forests are likely to use two correlated features evenly often whereas boosted models randomly chose one feature and then omit the other feature (see more details here [here](https://xgboost.readthedocs.io/en/stable/R-package/discoverYourData.html)). For the Mutli-layer Perceptron the importances are computed by permuting each predictor (one at a time) and computing the reduction in predictive performance (global model agnostic method). This method is nicely described in "interpretable machine learning" by Christoph Molnar [@molnar2019]. For linear models the importances simply correspond to the coefficients of the model.

---

# All Supplied Functions {.tabset}

## Random Forest

 - Short description of the algorithm: A classical ensemble model consisting
 of regression trees which are training with boostrapped samples (bagging). In
 addition the constitutent trees are further "decorrelated" by only considering
 a random subset of the features at each split (by default the squareroot of the number of predictors).

 - Unbiased predictions are based on out-of-bag (OOB) predictions.
 
 - Importances for each predictor for a given target a determined by averaging the decrease in RSS over all splits in which a predictor was used.

```{r}
random_forest_model
```

 - Running the model
 
```{r}
misty.views %>%
  run_misty(model.function = random_forest_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```
 
```{r echo=FALSE}
unlink("results")
```
 
## Gradient Boosting

 - Short description: Sequential training of weak learners (either regression trees or linear models) where the learner $m+1$ is fitted on the residuals of the model containing $m$ learners ($F_M$). The error can be minimized by exploiting the gradient of the loss function (squared residuals) with respect to the model $F_M$.

 - Unbiased predictions are based on k-fold cross-validation (aggregated 
 predictions for the holdout sets).
 
 - Importances for each predictor for a given target a determined by averaging the decrease in RSS over all splits in which a predictor was used (for tree gradient boosting).

```{r}
gradient_boosting_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = gradient_boosting_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

```{r echo=FALSE}
unlink("results")
```

 - Linear Gradient Boosting (here it is usually better to use more boosting iterations (`nrounds`) than 10).

```{r}
misty.views %>%
  run_misty(model.function = gradient_boosting_model,
            booster = "gblinear", nrounds = 20) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

```{r echo=FALSE}
unlink("results")
```

## MARS

 - MARS: Multivariate Adaptive Regression Splines

 - Short description: Each (included) predictor is split into a pair of 
 hinge functions which are then added to a linear regression model in the 
 forward pass. In the backward pass model terms are deleted based on the GCV
 (generalized cross-validation) (=prunning). However, one can also increase
 the degree of interaction (`degree`) such that the constructed features are
 not single hinge functions but products of 2 (or more) hinge functions.
 
  - Unbiased predictions are based on k-fold cross-validation (aggregated 
 predictions for the holdout sets)
 
 - The importances for each predictor are determined during the backward pass (when terms are deleted from the model). The decrease of the RSS is measured for each subset relative to the previous subset. Then, for each predictor the average decrease in RSS over all subsets in which the variable is included is used as importance. However if the `degree` hyperparamter is larger than one, the importance for a given model term containing several predictors is spread equally to all predictors.
 
```{r}
mars_model
```

```{r}
misty.views %>%
  run_misty(model.function = mars_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

```{r echo=FALSE}
unlink("results")
```

## Bagged MARS

 - MARS: Multivariate Adaptive Regression Splines
 
 - Short description: Each (included) predictor is split into a pair of 
 hinge functions which are then added to a linear regression model in the 
 forward pass. In the backward pass model terms are deleted based on the GCV
 (generalized cross-validation) (=prunning). However, one can also increase
 the degree of interaction (`degree`) such that the constructed features are
 not single hinge functions but products of 2 (or more) hinge functions.

 - Unbiased predictions are based on OOB predictions.
 
 - See importance extraction in MARS model above. Additionally the importances for each predictor are averaged over each constituent model.

```{r}
bagged_mars_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = bagged_mars_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

```{r echo=FALSE}
unlink("results")
```

## Linear Model

 - Short description: Simple multivariate linear regression. Fast runtimes and good interpretability, but cannot account for non-linear interactions.

 - Unbiased predictions are based on k-fold cross-validation (aggregated 
 predictions for the holdout sets)

```{r}
linear_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = linear_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

```{r echo=FALSE}
unlink("results")
```

## Linear Support Vector Machine

 - Short description: In regression setting the support vector machine
 can be framed as a robust regression technique that is less sensitive
 to outliers that linear regression (with squared deviation as loss function).
 The prediction is based on a weighted sum of the training instances that 
 are a certain threshold ($\epsilon$) away (= support vectors) from the 
 regression line and the loss function penalizes both the residuals and the 
sum of squared coefficients.

 - Unbiased predictions are based on k-fold cross-validation (aggregated 
 predictions for the holdout sets)
 
 - Importantly, we added an approximation argument to the implementation which basically says at what fraction of the training instances the SVM will be trained on (by default 40%). This drastically decreases the training time and does not seem to effect the performance to much.
 
  - By default only 40% of the training instances are used for training to increase the performance

```{r}
svm_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = svm_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

```{r echo=FALSE}
unlink("results")
```

## Multi-layer Perceptron

 - Short description: A small fully connected feedforward network, trained using standard backpropagation. Default activation function is logistic.

 - Unbiased predictions are based on k-fold cross-validation (aggregated predictions for the holdout sets)
 
 - Importantly, the feature importances is here calculated based on a global model agnostic method. More specifically each feature is permuted (one at a time) and then the reduction in predictive performance is used as measure for the variable importance. (This is achieved using the `iml` R package).
 
 - By default only 60% of the training instances are used for training to increase the performance

```{r}
mlp_model
```

 - Running the model:

```{r}
misty.views %>%
  run_misty(model.function = mlp_model,
            k = 10, approx.frac = 0.5) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
unlink("results")
```

---

# How to construct your own function

To construct a view-specific function let's first have a look at the input and then at the output of view-specific functions within the MISTy framework.

## Input

1. `view_data`: A tibble with each row corresponding to a spatial unit 
(e.g. single cell or Visium spot) and each column corresponding to a marker.

 * Due to an assert statements in `run_misty`, one can be sure that the 
variance of each target is non-zero.

 * We can for example look at the intraview form the example above.

```{r}
misty.views$intraview$data %>% slice_head(n=6)
```

2. `target`: String corresponding to the marker which should be modeled. (column name of the `view_data` tibble)

 * For example "ECM"

3. `seed`: Integer (passed down from `run_misty`)

 * The default seed is 42.

## Output

The output must be a `named list` comprising of:

1. `unbiased.predictions`: Tibble with one column called `index` for the "cell id" and another column called `prediction` for the unbiased prediction.

 * These unbiased predictions for the specified target can either come from the aggregated out-of-bag (OOB) predictions of a bagged ensemble model or the aggregated predictions for holdout sets of k-fold cross-validation.

 * The unbiased predictions are needed as input for the ridge regression late-fusion model, which combines the unbiased predictions from each view-specific model to assess the contribution from each view.

2. `importances` : Named Vector with the importances of the predictors as values and the names of the predictors as names (see the example below). Also see the paragraph about importances in the [General Remarks] section.

 * For example let's have a look at the result list returned by the ranger model.

```{r}
res.list <- random_forest_model(view_data = misty.views$intraview$data, 
                         target = "ECM", 
                         seed = 42)
str(res.list)
```

a) Top 6 entries of the `unbiased.predictions` tibble.

```{r}
res.list$unbiased.predictions %>% slice_head(n = 6)
```

b) Importances

```{r}
res.list$importances
```

## Example Function

To showcase how one can construct such a function, we will build a view-specific model based on a regression tree (implemented by `rpart`)

We will use the paraview from above as `view_data` and "ECM" as our `target`. So in general our input is going to look somehow like that. We have a tibble with the rows as spatial units (e.g. cells) and the columns are the markers (target or predictors).

```{r}
seed <- 42
target <- "ECM"
expr <- misty.views$intraview$data
target.vector <- expr %>% dplyr::pull(target)
view_data <- misty.views$paraview.10$data %>%
  mutate(!!target := target.vector)

view_data %>% slice_head(n=6)
```

We want to call `rpart` with this formula, specifying that every other marker should be used to predict the expression of our current target.

```{r}
as.formula(paste0(target, "~ ."))
```

```{r}
test.model <- rpart::rpart(formula = as.formula(paste0(target, "~ .")), 
                           data = view_data
)
summary(test.model)
```

Now to get unbiased predictions we need to perform cross validation and aggregate the predictions for the holdout instances.

First we will create 5 folds (`k=5`) using `caret::createFolds`

```{r}
seed <- 42
k <- 5
folds <- withr::with_seed(
  seed,
  caret::createFolds(seq.int(1, nrow(view_data)), k = k)
)
str(folds)
```

Next we will train a model for each fold and aggregate the prediction of the holdout instances.

```{r}
holdout.predictions <- purrr::map_dfr(folds, function(holdout) {
  
  in.fold <- seq.int(1, nrow(view_data))[!(seq.int(1, nrow(view_data)) %in% holdout)]
  
  train <- view_data[in.fold, ]
  test <- view_data[holdout, ]
  
  algo.arguments <- list(formula = as.formula(paste0(target, "~ .")), 
                        data = train)
  
  model <- do.call(rpart::rpart, algo.arguments)
  
  label.hat <- predict(model, test)
  
  tibble::tibble(index = holdout, prediction = label.hat)
}) %>% dplyr::arrange(index)
str(holdout.predictions)
```

As described above, apart from the unbiased predictions, we need the importances of the predictors. Therefore we will train one more model on the whole dataset and return the importances as named vector.

```{r}
algo.arguments.wm <- list(formula = as.formula(paste0(target, "~ .")), 
                          data = view_data)
  
whole.model <- do.call(rpart::rpart, algo.arguments.wm)

importances <- whole.model$variable.importance
importances
```

In the end we need to return everything in one list with the following names:

```{r}
list(unbiased.predictions = holdout.predictions, 
       importances = importances) %>% str # again added str for visualization
```

So up to this point our function would look like this.

```{r}
reg_tree_model_1 <- function(view_data, seed = 42, k = 5) {
  
  folds <- withr::with_seed(
  seed,
  caret::createFolds(seq.int(1, nrow(view_data)), k = k)
  )
  
  holdout.predictions <- purrr::map_dfr(folds, function(holdout) {
  
  in.fold <- seq.int(1, nrow(view_data))[!(seq.int(1, nrow(view_data)) %in% holdout)]
  
  train <- view_data[in.fold, ]
  test <- view_data[holdout, ]
  
  algo.arguments <- list(formula = as.formula(paste0(target, "~ .")), 
                        data = train)
  
  model <- do.call(rpart::rpart, algo.arguments)
  
  label.hat <- predict(model, test)
  
  tibble::tibble(index = holdout, prediction = label.hat)
}) %>% dplyr::arrange(index)
  
  algo.arguments.wm <- list(formula = as.formula(paste0(target, "~ .")), 
                          data = view_data)
  
  whole.model <- do.call(rpart::rpart, algo.arguments.wm)
  
  importances <- whole.model$variable.importance
  
  list(unbiased.predictions = holdout.predictions, 
       importances = importances)
}
```

And it works nicely.

```{r}
test <- reg_tree_model_1(view_data = view_data)
str(test)
```

Now in the final step we will add the possibility to supply ellipsis arguments
to the `rpart` model. The ellipsis arguments from the `run_misty()` call are passed down to the view-specifc model within the MISTy framework.

```{r}
reg_tree_model_2 <- function(view_data, seed = 42, k = 5, ...) {
  
  ellipsis.args <- list(...)
  
  folds <- withr::with_seed(
  seed,
  caret::createFolds(seq.int(1, nrow(view_data)), k = k)
  )
  
  holdout.predictions <- purrr::map_dfr(folds, function(holdout) {
  
  in.fold <- seq.int(1, nrow(view_data))[!(seq.int(1, nrow(view_data)) %in% holdout)]
  
  train <- view_data[in.fold, ]
  test <- view_data[holdout, ]
  
  algo.arguments <- list(formula = as.formula(paste0(target, "~ .")), 
                        data = train)
  
  if (!(length(ellipsis.args) == 0)) {
    algo.arguments <- merge_two(algo.arguments, ellipsis.args)
  }
  
  model <- do.call(rpart::rpart, algo.arguments)
  
  label.hat <- predict(model, test)
  
  tibble::tibble(index = holdout, prediction = label.hat)
}) %>% dplyr::arrange(index)
  
  algo.arguments.wm <- list(formula = as.formula(paste0(target, "~ .")), 
                          data = view_data)
  
  if (!(length(ellipsis.args) == 0)) {
    algo.arguments.wm <- merge_two(algo.arguments.wm, ellipsis.args)
  }
  
  whole.model <- do.call(rpart::rpart, algo.arguments.wm)
  
  importances <- whole.model$variable.importance
  
  list(unbiased.predictions = holdout.predictions, 
       importances = importances)
}
```

Now we check by supplying some parameters to `rpart.control`. For example changing the complexity parameter `cp` from 0.01 to 0.001. In regression setting this means that if a split does not decreae the $R^2$ by at least a factor of `cp`,  the split is not attempted.

```{r}
test2 <- reg_tree_model_2(view_data = view_data, cp = 0.001)
str(test2)
```

For the sake of it we could compare the performance of both test models.

```{r}
# Model 1
caret::defaultSummary(data = data.frame(obs = target.vector,
                                        pred = test$unbiased.predictions$prediction))
# Model 2
caret::defaultSummary(data = data.frame(obs = target.vector,
                                        pred = test2$unbiased.predictions$prediction))
```

```{r echo=FALSE}
unlink("results", recursive=TRUE)
```

---

# References
