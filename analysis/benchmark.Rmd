---
title: "Benchmark"
output:
  workflowr::wflow_html:
    toc: yes
    toc_float: yes
    toc_depth: 2
    code_folding: hide
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
csl: science.csl
---

Setup.

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = FALSE)
knitr::opts_knit$set(root.dir = "~/Saez/report")
```

```{r}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(slider))
```

# Introduction and Data {.tabset}

Loading the data.

```{r echo=FALSE}
all.files <-  list.files("data", full.names = TRUE)
res.files <- all.files[str_ends(all.files, "results.RDS")]
view.files <- all.files[str_ends(all.files, "views.RDS")]

# Getting the latest results and views
latest.result <- str_extract(res.files, "[0-9-]+") %>% 
  as.POSIXct() %>% 
  which.max()
latest.views <- str_extract(view.files, "[0-9-]+") %>% 
  as.POSIXct() %>% 
  which.max()

print(paste("Loading results from:", res.files[latest.result]))
all.results <- readRDS(res.files[latest.result])

print(paste("Loading views from:", view.files[latest.views]))
all.views <- readRDS(view.files[latest.views])
rm(all.files, res.files, view.files)
```

What data / experiments is the benchmarking based on?

```{r}
str_extract(names(all.results), "[^/]+(?=/)") %>% unique
```


```{r echo=FALSE}
# removing irrelevant data
views_to_remove <- c("/4iANCA/ratio_6_views", "/4iANCA/ratio_12_views",
                     "/4iANCA/ratio_24_views")
all.views <- all.views[(!names(all.views) %in% views_to_remove)]
```

## `synthetic`

-   Simulated data that are included in the MISTy package. As explained in the [Get Started Vignette](https://saezlab.github.io/mistyR/articles/mistyR.html),this dataset is based on a two-dimensional cellular automata model which models the production, diffusion, degradation and interactions of 11 molecular species in 4 different cell types.

-   Data Availability: `mistyR` package `data("synthetic")`

-   Features: Simulated Data

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/synthetic/l6`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/synthetic/l6`[[1]]$intraview$data)
```

-   Different views that were computed:

    +   `/synthetic/l6`: paraview computed with `l=6`

    +   `/synthetic/l12`: paraview computed with `l=12`

    +   `/synthetic/l24`: paraview computed with `l=24`

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/synthetic")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/synthetic/l6`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `mibi_tnbc`

-   Multiplexed Ion Beam Imaging (spatial proteomics) dataset from [A Structured Tumor-Immune Microenvironment in Triple Negative Breast Cancer Revealed by Multiplexed Ion Beam Imaging](https://dx.doi.org/10.1016/j.cell.2018.08.039) by Keren, L. et al. (Cell 174, 1373-1387.e19 (2018)) [@10.1016/j.cell.2018.08.039]

-   Data Availability: <https://mibi-share.ionpath.com/>

-   Features: Spatial Proteomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/mibi_tnbc/standard_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/mibi_tnbc/standard_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    -   `standard_views`: markers with zero variance were removed from each sample, juxtaview was computed with `neighbor.thr = 40`, and paraview was computed with `l = 120, zoi = 40`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/mibi_tnbc")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/mibi_tnbc/standard_views`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_preoptic`

-  MERFISH data from the murine hypothalamic preoptic region from [Molecular, spatial, and functional single-cell profiling of the hypothalamic preoptic region](https://dx.doi.org/10.1126/science.aau5324) by Moffitt, R. et al. (Science 362, 6416 (2018))

-   Data Availability: [Spatial DB](http://www.spatialomics.org/SpatialDB/merfish_30385464_browse.php)

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_preoptic/hvg_views`)
```

-   Number of markers in each sample (depending on the view, see explanation below):

```{r echo=FALSE}
c(ncol(all.views$`/merfish_preoptic/hvg_views`[[1]]$intraview$data),
  ncol(all.views$`/merfish_preoptic/standard_views`[[1]]$intraview$data))
```

-   Different views that were computed:

    +   `standard_views`: markers with zero variance were removed from each sample, the juxtaview was computed with `neighbor.thr = 30`, and the paraview with `l = 120, zoi = 30`.

    +   `hvg_views`: markers with zero variance were removed from each sample, the 108 most spatially variable genes (computed with *SPARK-X* [@10.1186/s13059-021-02404-0]) were selected, the juxtaview was computed with `neighbor.thr = 30`, and the paraview with `l = 120, zoi = 30`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_preoptic")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_preoptic/hvg_views`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_liver`

-   MERFISH data from the murine fetal liver from [Spatial transcriptome profiling by MERFISH reveals fetal liver hematopoietic stem cell niche architecture](https://dx.doi.org/10.1038/s41421-021-00266-1) by Yanfang, L. et al. (Cell Discovery 7, 47 (2021))

-   Data Availability: [SIYUAN WANG LAB](https://campuspress.yale.edu/wanglab/HSCMERFISH/)

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_liver/standard280_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/merfish_liver/standard280_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    +   `standard280_views`: markers with zero variance were removed from each sample, samples with fewer than 280 cells were removed, the juxtaview was computed with `neighbor.thr = 100`, and the paraview with `l = 180, zoi = 100`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_liver")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_liver/standard280_views`,
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_bc`

-   MERFISH data from metastatic breast cancer (unpublished)

-   Data Availability: -

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_bc/hvg_views`)
```

-   Number of markers in each sample (depending on the view, see explanation below):

```{r echo=FALSE}
c(ncol(all.views$`/merfish_bc/hvg_views`[[1]]$intraview$data),
  ncol(all.views$`/merfish_bc/standard_views`[[1]]$intraview$data))
```

-   Different views that were computed:

    +   `standard_views`: markers with zero variance were removed from each sample, the juxtaview was computed with `neighbor.thr = 15`, and the paraview with `l = 100, zoi = 15`.

    +   `hvg_views`: markers with zero variance were removed from each sample, the 86 most spatially variable genes (computed with *SPARK-X* [@10.1186/s13059-021-02404-0]) were selected, the juxtaview was computed with `neighbor.thr = 15`, and the paraview with `l = 100, zoi = 15`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_bc")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_bc/hvg_views`,
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `4iANCA`

-   Iiterative Indirect Immunofluorescence Imaging (4i) data from glomeruli (unpublished)

-   Data Availability: -

-   Features: Spatial Proteomics, Single Cell Resolution, No Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/4iANCA/ratio_153_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/4iANCA/ratio_153_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    + `/4iANCA/ratio_153_views`

    + `/4iANCA/ratio_76_views`

    + `/4iANCA/ratio_38_views`

    + `/4iANCA/ratio_19_views`

    + `/4iANCA/ratio_9\_views`

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/4iANCA")]
```

-   Number of spatial units (e.g. cells) per sample, depending on the computed view:

    -   Number of cells per sample: No segmentation was performed, therefore the pixels were simply binned with 5 different schemes. The pixels have a side length of $0.13$ microns, and an average eukaryotic cell is between $10$ and $100 \micro m$ in diameter. Therefore I choose the largest binning to be `floor(20/0.13) = 153`, meaning the side length of one spatial unit (bin) will correspond to 153 pixels. The smaller bins are then sequentially half of the size (76, 38, 19, 9). The corresponding parameter for the computation of the paraview was adjusted accordingly, meaning if the l parameter $l_{153}$ was used for computation of the paraview for the 153 binning, $2 \times l_{153}$ was used for the 76 binning.

```{r echo=FALSE}
map2_dfr(all.views[str_starts(names(all.views), "/4iANCA")], 
        names(all.views)[str_starts(names(all.views), "/4iANCA")],
        function(view, n) {
    map_dfr(view, ~ nrow(.x$intraview$data)) %>%
    mutate(experiment = n)
}) %>%
  pivot_longer(cols = !c(experiment), 
               names_to = "sample", 
               values_to = "n") %>%
  group_by(experiment) %>%
  summarise(mean_n = mean(n)) %>%
  arrange(mean_n)
```

------------------------------------------------------------------------

# Scripts

The scripts that were used for all analysis can be found in this [GitHub repository](https://github.com/schae211/report/tree/master/cluster_scripts). The computations were performed on a cluster managed by Slurm.

------------------------------------------------------------------------

# Comparing Performances {.tabset}

Define a function to get clean data for a particular study.

```{r}
# helper function to get all misty runs for a particular study
filter_runs <- function(all.runs, str.study, views) {
  str.view <- paste(views, collapse="|")
  all.names <- names(all.runs)
  names.study <- all.names[str_detect(all.names, str.study)]
  names.view <- names.study[str_detect(names.study, str.view)]
  sucessful.runs <- map_lgl(all.results[names.view], ~ typeof(.x) == "list")
  all.results[names.view][sucessful.runs]
}
```

Define a function to extract the improvements (performance measures) from
a list of misty results.

```{r}
get_performance <- function(filtered.run) {
  map2_dfr(filtered.run, names(filtered.run),
                                   function(misty.run, name) {
    misty.run$improvements %>%
      mutate(algorithm = str_extract(name, "(?<=/)[^/]+$")) %>%
      mutate(study = str_extract(sample, "(?<=OUTPUT/)[^/]+")) %>%
      mutate(view = str_extract(sample, paste0("(?<=OUTPUT/", study, "/)[^/]+")))
  })
}
```

Define a function to check whether something is missing in misty performance
files.

```{r}
check_runs <- function(misty.performances, ordering = NULL) {
  
    levs <- function() {
    if (is.null(ordering)) {
      out <- misty.performances %>% 
        filter(!(algorithm %in% exclude)) %>%
        pull(algorithm) %>%
        unique()
    } else {
      out <- ordering
    }
    out
  }
  
  misty.performances %>%
    select(algorithm, sample, view) %>%
    distinct() %>%
    mutate(algorithm = factor(algorithm, levels = levs())) %>%
    group_by(view, algorithm) %>%
    summarise(sample_number = n(), .groups="drop") %>%
    pivot_wider(names_from = "view", values_from = "sample_number")
}
```

Define a function to plot misty performances for all targets and for all 
considered algorithms (and for all views that were included).

```{r}
plot_performance <- function(misty.performances, str.measure, 
                             str.summary = "mean", targets = FALSE,
                             exclude = c(), wrap.views = FALSE,
                             ncol = 2, ordering = NULL) {
  
  str.summary = match.arg(str.summary, c("mean", "median"))
  
  fill = ifelse(str.summary == "mean", "mean", "median")
  
  theme_custom <- function() {
    if (targets) {
      out <- theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5))
    } else {
      out <- theme(axis.text.y = element_blank(), axis.ticks.y=element_blank(),
                   axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5))
    }
    out
  }
  
  wrap <- function() {
    if (wrap.views) {
      out <- facet_wrap(~ view, ncol = ncol)
    } else {
      out <- NULL
    }
    out
  }
  
  levs <- function() {
    if (is.null(ordering)) {
      out <- misty.performances %>% 
        filter(!(algorithm %in% exclude)) %>%
        pull(algorithm) %>%
        unique()
    } else {
      out <- ordering
    }
    out
  }
  
  plot <- misty.performances %>%
    filter(measure == str.measure) %>%
    filter(!(algorithm %in% exclude)) %>%
    mutate(algorithm = factor(algorithm, levels=levs())) %>%
    group_by(view, algorithm, target) %>%
    summarise(mean = mean(value), median = median(value), 
              sd = sd(value), .groups = "drop_last") %>%
    ggplot() +
    geom_tile(aes(x = algorithm, y = target, 
                  # use .data[[var]] to access data variables with strings
                  fill = .data[[fill]] )) +
    scale_fill_viridis_c(values = c(0, 0.1, 0.2, 0.4, 0.7, 1)) +
    labs(x = "Algorithm", fill = paste0(str.summary, " : ", str.measure)) +
    theme_custom() +
    wrap()
  
  print(plot)
}
```

```{r}
# default ordering of the algorithms
def.ord <-  c("RF", "TBOOST", "MARS100", "MARS", "MARS80", "MARS60", "MARS40",
              "BGMARS", "LBOOST", "LM", "SVM", "MLP")
```


## `synthetic`

```{r}
filter_runs(all.results, "synthetic", c("l6", "l12", "l24")) %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```


```{r fig.width=8, fig.height=4}
filter_runs(all.results, "synthetic", c("l6", "l12", "l24")) %>%
  get_performance() %>%
  mutate(view = factor(view, levels = c("l6", "l12", "l24"))) %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = TRUE, wrap.views = TRUE, ncol=3,
                   ordering = def.ord) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = TRUE, wrap.views = TRUE, ncol=3,
                   ordering = def.ord) %>%
  invisible()
```

## `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

```{r fig.width=8}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = TRUE, ordering = def.ord) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = TRUE, ordering = def.ord) %>%
  invisible()
```

---

## `merfish_preoptic`

We see that MARS (with the current configuration) is not scalable enough to
handle such large datasets with 100 markers and up to 70,000 spatial units.

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

```{r fig.width=8}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord,
                   exclude = c("MARS", "MARS40", "MARS60", "MARS80")) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord,
                   exclude = c("MARS", "MARS40", "MARS60", "MARS80")) %>%
  invisible()
```

---

## `merfish_liver`

For some reason everything but bagged MARS, MARS, and gradient boosted trees 
failed.

```{r}
filter_runs(all.results, "merfish_liver", "std280") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

```{r fig.width=8}
filter_runs(all.results, "merfish_liver", "std280") %>%
  get_performance() %T>%
  plot_performance(str.measure = "mulit.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord) %>%
  invisible()
```

---

## `merfish_bc`

Since the bagged MARS algorithm could only finish the computation for a single
sample I must exclude the result. One should also keep in mind that the 
MARS algorithm was not able to finish (2 samples are missing).

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

```{r fig.width=8}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = FALSE, exclude = c("BGMARS"),
                   ordering = def.ord) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, exclude = c("BGMARS"),
                   ordering = def.ord) %>%
  invisible()
```

---

## `4iANCA`

The computation for the smallest binning (`ratio_9`) could not be finished.

Also why do we only have 10 samples instead of 36 samples for ratio 153?

```{r}
filter_runs(all.results, "4iANCA", c("ratio_153", "ratio_76", 
                                     "ratio_38", "ratio_19", "ratio_9")) %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

```{r fig.width=10, fig.height=6}
filter_runs(all.results, "4iANCA", c("ratio_153", "ratio_76", "ratio_38",
                                     "ratio_19", "ratio_9")) %>%
  get_performance() %>%
  mutate(view = factor(view, levels=c("ratio_153", "ratio_76", "ratio_38",
                                     "ratio_19", "ratio_9"))) %T>%
  plot_performance(str.measure = "multi.R2", wrap.views = TRUE, ncol=3,
                   ordering = def.ord) %>%
  plot_performance(str.measure = "gain.R2", wrap.views = TRUE, ncol=3,
                   ordering = def.ord)
```

---

# Performance Figure

Plot the performances of the different algorithms for all studies in one figure
using only one view from each study.

```{r}

```


# Comparing Importances {.tabset}

Define function to extract the importances (interaction measures).

```{r}
get_importance <- function(filtered.run) {
  map2_dfr(filtered.run, names(filtered.run), function(misty.run, name) {
                                     
    # using str_extract_all drastically improves the performance
    # warning: indices of algorithm, study, and view.comp hardcoded
    all.info <- str_extract_all(misty.run$importances$sample, "(?<=/)[^/]+")
                                     
    misty.run$importances %>%
      mutate(algorithm = map_chr(all.info, ~ .x[length(.x)-1])) %>%
      mutate(study = map_chr(all.info, ~ .x[length(.x)-3])) %>%
      mutate(view.comp = map_chr(all.info, ~ .x[length(.x)-2]))
  }) %>%
    mutate(sample = str_extract(sample, "[^/]+$"))
}
```

Define function to check whether all importances are present for each algorithm

```{r}
check_algs <- function(misty.performances) {
  misty.performances %>%
    unite(relation, Predictor, Target, sep="_") %>%
    select(-c(study, view.comp)) %>%
    drop_na() %>%
    pivot_wider(names_from=relation, values_from=Importance) %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    pivot_longer(cols = !c(sample, view, algorithm)) %>%
    mutate(check.NA = is.na(value)) %>%
    group_by(algorithm) %>%
    summarise(n.NAs = sum(check.NA), n = n())
}
```

Define function to get splits according to view and sample from misty importances.

```{r}
split_importances <- function(misty.importances) {
  tidy.df <- misty.importances %>%
    unite(relation, Predictor, Target, sep="_") %>%
    drop_na() %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    pivot_wider(names_from = algorithm, values_from = Importance) %>%
    replace(is.na(.), 0) %>%
    group_by(view, sample)
  
  list(splits = group_split(tidy.df), keys = group_keys(tidy.df))
}
```

Define function to compute the cosine similarity given a tibble of vectors, where
each column represents a vector. Indices are harcoded.

```{r}
cosine_similarity <- function(imp.tbl, relative = "RF") {
  comp <- imp.tbl %>% pull(relative)
  map_dbl(imp.tbl[-c(1:5)], function(vec) {
    cosine.sim <- (comp %*% vec * 1/(comp %*% comp)**0.5 * 1/(vec %*% vec)**0.5)
  })
}
```

Define function to compute the jaccard index given a tibble of vectors, where
each column represents a vector. Indices are harcoded.

```{r}
jaccard_index <- function(imp.tbl, relative = "RF", cutoff = 0.5) {
  comp <- imp.tbl %>% filter(.data[[relative]] > cutoff) %>% pull(relation)
  algs <- names(imp.tbl)[-c(1:5)] %>% setNames(names(imp.tbl)[-c(1:5)])
  
  map_dbl(algs, function(alg) {
    vec <- imp.tbl %>% filter(.data[[alg]] > cutoff) %>% pull(relation)
    length(intersect(comp, vec)) / length(union(comp, vec))
  })
}
```

Define wrapper function to compute comparitive statistics (e.g. cosine similarity,
jaccard index), given a misty importances tibble.

First, the tibble containing the importances is split according to views and
samples. Second, for each sub tibble the 

```{r}
wrap_function <- function(misty.importances, f, ...) {
  
  ellipsis.args <- list(...)
  # Split the importance tibble corresponding to views and samples
  # since we want to compare the algorithm for the same sample and view
  split.list <- split_importances(misty.importances)
  
  # 1. map over each view
  map_dfr(unique(split.list$keys$view), function(sel.view) {
    
    # 2. Map over each sample corresponding to this view
    split.list$keys %>%
      mutate(id = row_number()) %>%
      filter(view == sel.view) %>%
      pull(id, name = sample) %>%
      map_dfr(function(tbl.id) {
        
        # 3. Compute cosine similarity (and add corresponding view and sample)
        
        args <- rlist::list.merge(list(imp.tbl = split.list$splits[[tbl.id]]), 
                                  ellipsis.args)
        c(do.call(f, args), "id" = tbl.id)
          
      }) %>% mutate(view = sel.view,
                    sample = split.list$keys$sample[as.integer(id)]) 
  }) %>%
    pivot_longer(-c(id, view, sample), 
                 names_to = "algorithm", values_to = "cosine") %>%
    
      # 4. Group by algorithm 
      # and get the mean of the cosine similarity for each sample
      group_by(algorithm, view) %>%
      summarise(mean = mean(cosine), sd = sd(cosine), .groups="drop") %>%
      arrange(algorithm)
}
```

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  wrap_function(jaccard_index, relative="RF", cutoff=0.5)
```


Function to compute the cosine similarity for each algorithm 
for all views and samples.

```{r}
cosine_similarity_wrapper <- function(misty.importances, relative = "RF") {
  
  # Split the importance tibble corresponding to views and samples
  # since we want to compare the algorithm for the same sample and view
  split.list <- split_importances(misty.importances)
  
  # 1. map over each view
  map_dfr(unique(split.list$keys$view), function(sel.view) {
    
    # 2. Map over each sample corresponding to this view
    split.list$keys %>%
      mutate(id = row_number()) %>%
      filter(view == sel.view) %>%
      pull(id, name = sample) %>%
      map_dfr(function(tbl.id) {
        
        # 3. Compute cosine similarity (and add corresponding view and sample)
        c(cosine_similarity(split.list$splits[[tbl.id]], relative), "id" = tbl.id)
          
      }) %>% mutate(view = sel.view,
                    sample = split.list$keys$sample[as.integer(id)]) 
  }) %>%
    pivot_longer(-c(id, view, sample), 
                 names_to = "algorithm", values_to = "cosine") %>%
    
      # 4. Group by algorithm 
      # and get the mean of the cosine similarity for each sample
      group_by(algorithm, view) %>%
      summarise(mean = mean(cosine), sd = sd(cosine), .groups="drop") %>%
      arrange(algorithm)
}
```

```{r}
plot_cosine_similarity <- function(cosine_similarity) {
  cosine_similarity %>%
    mutate(low = mean-sd, high = mean+sd) %>%
    mutate(algorithm = factor(algorithm, levels = def.ord)) %>%
    ggplot() +
    geom_point(aes(x = algorithm, y = mean)) +
    geom_errorbar(aes(x = algorithm, ymin = low, ymax = high)) +
    facet_wrap(~ view) +
    theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5)) +
    lims(y = c(0, 1))
}
```

```{r}
jaccard_index_wrapper <- function(misty.importances, relative = "RF", cutoff = 0.5) {
  
  # Split the importance tibble corresponding to views and samples
  # since we want to compare the algorithm for the same sample and view
  split.list <- split_importances(misty.importances)
  
  # 1. map over each view
  map_dfr(unique(split.list$keys$view), function(sel.view) {
    
    # 2. Map over each sample corresponding to this view
    split.list$keys %>%
      mutate(id = row_number()) %>%
      filter(view == sel.view) %>%
      pull(id, name = sample) %>%
      map_dfr(function(tbl.id) {
        
        # 3. Compute jaccard index (and add corresponding view and sample)
        c(jaccard_index(split.list$splits[[tbl.id]], relative, cutoff), "id" = tbl.id)
          
      }) %>% mutate(view = sel.view,
                    sample = split.list$keys$sample[as.integer(id)]) 
  }) %>%
    pivot_longer(-c(id, view, sample), 
                 names_to = "algorithm", values_to = "jaccard") %>%
    
      # 4. Group by algorithm 
      # and get the mean of the cosine similarity for each sample
      group_by(algorithm, view) %>%
      summarise(mean = mean(jaccard), sd = sd(jaccard), .groups="drop") %>%
      arrange(algorithm)
}
```

```{r}
plot_jaccard <- function(jaccard) {
  jaccard %>%
    mutate(low = mean-sd, high = mean+sd) %>%
    mutate(algorithm = factor(algorithm, levels = def.ord)) %>%
    ggplot() +
    geom_point(aes(x = algorithm, y = mean)) +
    geom_errorbar(aes(x = algorithm, ymin = low, ymax = high)) +
    facet_wrap(~ view) +
    theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5)) +
    lims(y = c(0, 1))
}
```

## `synthetic`

For the boosting functions several importances are not returned?

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  check_algs()
```

### Cosine Similarity

Now let's look at the cosine similarities.

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  cosine_similarity_wrapper()
```

And plot them.

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  cosine_similarity_wrapper() %>%
  plot_cosine_similarity()
```

### Jaccard Index after Binarization

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  jaccard_index_wrapper(cutoff = 0) %>%
  plot_jaccard()
```

### Pearson Correlation

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  unite(relation, Predictor, Target) %>%
  drop_na() %>%
  pivot_wider(names_from = relation, values_from = Importance) %>%
  filter(view == "intra") %>%
  select(-c(study, view.comp)) %>%
  mutate(sample = str_extract(sample, "[^/]+$")) %>%
  group_by(algorithm) %>%
  summarise(across(4:112, mean)) %>%
  replace(is.na(.), 0) %>%
  pivot_longer(cols = !algorithm) %>%
  pivot_wider(names_from = algorithm, values_from = value) %>%
  select(-name) %>%
  cor() %>% 
  corrplot::corrplot(method="color", col.lim = c(0,1), 
      col = colorRampPalette(c("#414487FF", "#414487FF", "#414487FF",
                               "#414487FF", "#22A884FF", "#7AD151FF",
                               "#FDE725FF"))(100),
      tl.col = "black")
```


```{r fig.width=6, fig.height=6}
tidy.df <- filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  unite(relation, Predictor, Target) %>%
  drop_na() %>%
  pivot_wider(names_from = relation, values_from = Importance) %>%
  filter(view == "intra") %>%
  select(-c(study, view.comp)) %>%
  mutate(sample = str_extract(sample, "[^/]+$")) %>%
  group_by(algorithm) %>%
  summarise(across(4:112, mean)) %>%
  replace(is.na(.), 0) %>%
  pivot_longer(cols = !algorithm) %>%
  pivot_wider(names_from = algorithm, values_from = value) %>%
  rename(importance = name) %>%
  pivot_longer(cols = !c(importance, RF)) %>%
  mutate(name = factor(name))

ggplot() +
  geom_point(aes(x = tidy.df$RF, y = tidy.df$value, col = tidy.df$name)) +
  geom_line(aes(x = tidy.df$RF, y = tidy.df$RF), col = "blue") +
  coord_fixed() +
  scale_color_brewer(palette="Dark2")
```


## `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  check_algs()
```

### Cosine Similarity

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  cosine_similarity_wrapper() %>%
  plot_cosine_similarity()
```


## `merfish_preoptic`

### Cosine Similarity

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  check_algs()
```

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  cosine_similarity_wrapper() %>%
  plot_cosine_similarity()
```

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  jaccard_index_wrapper() %>%
  plot_jaccard()
```

## `merfish_liver`

## `merfish_bc`

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  check_algs()
```

### Cosine Similarity

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  cosine_similarity_wrapper() %>%
  plot_cosine_similarity()
```

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  jaccard_index_wrapper() %>%
  plot_jaccard()
```

## `4iANCA`

```{r}
filter_runs(all.results, "4iANCA", "ratio_153") %>%
  get_importance() %>%
  check_algs()
```

### Cosine Similarity

```{r}
filter_runs(all.results, "4iANCA", "ratio_153") %>%
  get_importance() %>%
  cosine_similarity_wrapper() %>%
  plot_cosine_similarity()
```

```{r}
filter_runs(all.results, "4iANCA", "ratio_153") %>%
  get_importance() %>%
  jaccard_index_wrapper() %>%
  plot_jaccard()
```

# Comparing Runtimes

Small timing benchmark based on the whole `synthetic` dataset, using the view
composition: intraview and paraview with `l = 12` and executing the code
with 12 workers.

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
plan("multisession", workers=12)

c("RF" = random_forest_model, "TBOOST" = gradient_boosting_model, 
  "MARS" = mars_model, "BGMARS" = bagged_mars_model,
  "LM" = linear_model, "SVM" = svm_model, "MLP" = mlp_model) %>%
  imap_dfr(function(fun, fun.name) {
    start <- Sys.time()
    misty.results.path <- 
      imap_chr(all.views$`/synthetic/l12`, function(misty.views, name) {
        misty.views %>%
          run_misty(model.function = fun, results.folder = paste0("results/", name))
    })
    end <- Sys.time()
    unlink("results", recursive = TRUE)
    tibble::tibble(model.function = fun.name, time = (end - start))
  }) %>%
  cbind("relative_to_RF" = as.double(.$time) / 
          as.double(rep(.$time[.$model.function=="RF"], length(.$model.function)))) %>%
  arrange(time)
```

As expected the linear model is by far the fastest model. Random forest
and gradient boosting with trees are equally fast and perform much better than
the multi-layer perceptron, bagged mars models, and the support vector machine.


# Hyperparameter Optimization {.tabset}

## RF

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "RF_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "RF_hyper_")) %>%
  select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("n.tree", "min.node.size", "max.depth", "splitrule"), "_") %>%
  select(name, everything())
```


```{r}
filter_runs(all.results, "mibi_tnbc", "RF_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "RF_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

We can clearly see that `max.depths = 1` reduces the performance significantly.

## MARS

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("penalty", "degree", "fast.k", "pmethod"), "_") %>%
  select(name, everything())
```

```{r}
filter_runs(all.results, "mibi_tnbc", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

Interestingly the performance seems to be pretty much invariant to the choice
of hyperparameters here.

### `synthetic`

```{r}
filter_runs(all.results, "synthetic", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("penalty", "degree", "fast.k", "pmethod"), "_") %>%
  select(name, everything())
```

```{r}
filter_runs(all.results, "synthetic", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  plot_performance(str.measure="multi.R2", targets = TRUE)
```

For the synthetic datasets we clearly see that a higher degree increases the
model performance.

What about the timing?

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
plan("multisession", workers=12)

# Penalties: -1, 0, 2, 3
penalties <- c(-1, 0, 2, -1, 0, 2, 3, 4, 3, 3, 3, 3)

# Degrees: 1, 2, 3
degrees <- c(1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2)

# Fast K: 20, 10, 5
fastks <- c(20, 20, 20, 20, 20, 20, 20, 20, 10, 5, 20, 20)

# Prunning Method: "backward", "none", "forward"
pmethods <- c("backward", "backward", "backward", "backward", "backward", 
              "backward", "backward", "backward", "backward", "backward", 
              "none", "forward")

test <- pmap_dfr(list(penalties, degrees, fastks, pmethods),
         function(penalty, degree, fast.k, pmethod) {
           print(paste(penalty, degree, fast.k, pmethod, sep="_"))
           start <- Sys.time()
           
           walk(all.views$`/synthetic/l12`, function(view) {
             run_misty(views = view, model.function = mars_model,
                       degree = degree, penalty = penalty, 
                       fast.k = fast.k, pmethod = pmethod)
           })
           
           end <- Sys.time()
           unlink("results")
           tibble(name = paste(penalty, degree, fast.k, pmethod, sep="_"),
                  penalty = penalty, degree = degree, fast.k = fast.k, pmethod = pmethod,
                  time = end - start)
         }) %>%
  arrange(time)
```


## Linear Boosting

```{r}
filter_runs(all.results, "mibi_tnbc", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("lambda", "alpha"), "_") %>%
  select(name, everything())
```

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

No penalty leads to the best results.

## Tree Boosting

```{r}
filter_runs(all.results, "mibi_tnbc", "TBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "TBOOST_hyper_")) %>%
  select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("eta", "max_depth", "min_child_weight", "subsample"), "_") %>%
  select(name, everything())
```

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "TBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "TBOOST_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

Similarly to the random forest above, we see that `max_depth = 1` reduces the
overall performance.

# References
