---
title: "Benchmark"
output:
  workflowr::wflow_html:
    toc: yes
    toc_float: yes
    toc_depth: 2
    code_folding: hide
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
csl: science.csl
---

Setup.

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = FALSE)
knitr::opts_knit$set(root.dir = "~/Saez/report")
```

```{r}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(slider))
```

# Introduction and Data {.tabset}

Loading the data.

```{r echo=FALSE}
all.files <-  list.files("data", full.names = TRUE)
res.files <- all.files[str_ends(all.files, "results.RDS")]
view.files <- all.files[str_ends(all.files, "views.RDS")]

# Getting the latest results and views
latest.result <- str_extract(res.files, "[0-9-]+") %>% 
  as.POSIXct() %>% 
  which.max()
latest.views <- str_extract(view.files, "[0-9-]+") %>% 
  as.POSIXct() %>% 
  which.max()

print(paste("Loading results from:", res.files[latest.result]))
all.results <- readRDS(res.files[latest.result])

print(paste("Loading views from:", view.files[latest.views]))
all.views <-readRDS(view.files[latest.views])
```

What data / experiments is the benchmarking based on?

```{r}
all.names <- names(all.results)
experiments <- str_extract(all.names, "[^/]+(?=/)") %>% unique
experiments
```


```{r echo=FALSE}
# removing irrelevant data
views_to_remove <- c("/4iANCA/ratio_6_views", "/4iANCA/ratio_12_views",
                     "/4iANCA/ratio_24_views")
all.views <- all.views[(!names(all.views) %in% views_to_remove)]
```

## `synthetic`

-   Simulated data that are included in the MISTy package. As explained in the [Get Started Vignette](https://saezlab.github.io/mistyR/articles/mistyR.html),this dataset is based on a two-dimensional cellular automata model which models the production, diffusion, degradation and interactions of 11 molecular species in 4 different cell types.

-   Data Availability: `mistyR` package `data("synthetic")`

-   Features: Simulated Data

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/synthetic/l6`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/synthetic/l6`[[1]]$intraview$data)
```

-   Different views that were computed:

    +   `/synthetic/l6`: paraview computed with `l=6`

    +   `/synthetic/l12`: paraview computed with `l=12`

    +   `/synthetic/l24`: paraview computed with `l=24`

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/synthetic")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/synthetic/l6`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `mibi_tnbc`

-   Multiplexed Ion Beam Imaging (spatial proteomics) dataset from [A Structured Tumor-Immune Microenvironment in Triple Negative Breast Cancer Revealed by Multiplexed Ion Beam Imaging](https://dx.doi.org/10.1016/j.cell.2018.08.039) by Keren, L. et al. (Cell 174, 1373-1387.e19 (2018)) [@10.1016/j.cell.2018.08.039]

-   Data Availability: <https://mibi-share.ionpath.com/>

-   Features: Spatial Proteomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/mibi_tnbc/standard_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/mibi_tnbc/standard_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    -   `standard_views`: markers with zero variance were removed from each sample, juxtaview was computed with `neighbor.thr = 40`, and paraview was computed with `l = 120, zoi = 40`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/mibi_tnbc")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/mibi_tnbc/standard_views`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_preoptic`

-  MERFISH data from the murine hypothalamic preoptic region from [Molecular, spatial, and functional single-cell profiling of the hypothalamic preoptic region](https://dx.doi.org/10.1126/science.aau5324) by Moffitt, R. et al. (Science 362, 6416 (2018))

-   Data Availability: [Spatial DB](http://www.spatialomics.org/SpatialDB/merfish_30385464_browse.php)

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_preoptic/hvg_views`)
```

-   Number of markers in each sample (depending on the view, see explanation below):

```{r echo=FALSE}
c(ncol(all.views$`/merfish_preoptic/hvg_views`[[1]]$intraview$data),
  ncol(all.views$`/merfish_preoptic/standard_views`[[1]]$intraview$data))
```

-   Different views that were computed:

    +   `standard_views`: markers with zero variance were removed from each sample, the juxtaview was computed with `neighbor.thr = 30`, and the paraview with `l = 120, zoi = 30`.

    +   `hvg_views`: markers with zero variance were removed from each sample, the 108 most spatially variable genes (computed with *SPARK-X* [@10.1186/s13059-021-02404-0]) were selected, the juxtaview was computed with `neighbor.thr = 30`, and the paraview with `l = 120, zoi = 30`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_preoptic")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_preoptic/hvg_views`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_liver`

-   MERFISH data from the murine fetal liver from [Spatial transcriptome profiling by MERFISH reveals fetal liver hematopoietic stem cell niche architecture](https://dx.doi.org/10.1038/s41421-021-00266-1) by Yanfang, L. et al. (Cell Discovery 7, 47 (2021))

-   Data Availability: [SIYUAN WANG LAB](https://campuspress.yale.edu/wanglab/HSCMERFISH/)

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_liver/standard280_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/merfish_liver/standard280_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    +   `standard280_views`: markers with zero variance were removed from each sample, samples with fewer than 280 cells were removed, the juxtaview was computed with `neighbor.thr = 100`, and the paraview with `l = 180, zoi = 100`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_liver")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_liver/standard280_views`,
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_bc`

-   MERFISH data from metastatic breast cancer (unpublished)

-   Data Availability: -

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_bc/hvg_views`)
```

-   Number of markers in each sample (depending on the view, see explanation below):

```{r echo=FALSE}
c(ncol(all.views$`/merfish_bc/hvg_views`[[1]]$intraview$data),
  ncol(all.views$`/merfish_bc/standard_views`[[1]]$intraview$data))
```

-   Different views that were computed:

    +   `standard_views`: markers with zero variance were removed from each sample, the juxtaview was computed with `neighbor.thr = 15`, and the paraview with `l = 100, zoi = 15`.

    +   `hvg_views`: markers with zero variance were removed from each sample, the 86 most spatially variable genes (computed with *SPARK-X* [@10.1186/s13059-021-02404-0]) were selected, the juxtaview was computed with `neighbor.thr = 15`, and the paraview with `l = 100, zoi = 15`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_bc")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_bc/hvg_views`,
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `4iANCA`

-   Iiterative Indirect Immunofluorescence Imaging (4i) data from glomeruli (unpublished)

-   Data Availability: -

-   Features: Spatial Proteomics, Single Cell Resolution, No Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/4iANCA/ratio_153_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/4iANCA/ratio_153_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    +   /4iANCA/ratio_153_views

    +   /4iANCA/ratio_76_views

    +   /4iANCA/ratio_38_views

    +   /4iANCA/ratio_19_views

    +   /4iANCA/ratio_9\_views

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/4iANCA")]
```

-   Number of spatial units (e.g. cells) per sample, depending on the computed view:

    -   Number of cells per sample: No segmentation was performed, therefore the pixels were simply binned with 5 different schemes. The pixels have a side length of $0.13$ microns, and an average eukaryotic cell is between $10$ and $100 \micro m$ in diameter. Therefore I choose the largest binning to be `floor(20/0.13) = 153`, meaning the side length of one spatial unit (bin) will correspond to 153 pixels. The smaller bins are then sequentially half of the size (76, 38, 19, 9). The corresponding parameter for the computation of the paraview was adjusted accordingly, meaning if the l parameter $l_{153}$ was used for computation of the paraview for the 153 binning, $2 \times l_{153}$ was used for the 76 binning.

```{r echo=FALSE}
map2_dfr(all.views[str_starts(names(all.views), "/4iANCA")], 
        names(all.views)[str_starts(names(all.views), "/4iANCA")],
        function(view, n) {
    map_dfr(view, ~ nrow(.x$intraview$data)) %>%
    mutate(experiment = n)
}) %>%
  pivot_longer(cols = !c(experiment), 
               names_to = "sample", 
               values_to = "n") %>%
  group_by(experiment) %>%
  summarise(mean_n = mean(n)) %>%
  arrange(mean_n)
```

------------------------------------------------------------------------

# Scripts

The scripts that were used for all analysis can be found in this [GitHub repository](https://github.com/schae211/report/tree/master/cluster_scripts). The computations were performed on a cluster managed by Slurm.

------------------------------------------------------------------------

# Comparing Performances {.tabset}

Function to get clean data for a particular study.

```{r}
# helper function to get all misty runs for a particular study
filter_runs <- function(all.runs, str.study, views) {
  str.view <- paste(views, collapse="|")
  all.names <- names(all.runs)
  names.study <- all.names[str_detect(all.names, str.study)]
  names.view <- names.study[str_detect(names.study, str.view)]
  sucessful.runs <- map_lgl(all.results[names.view], ~ typeof(.x) == "list")
  all.results[names.view][sucessful.runs]
}
```

Function to extract the improvements (performance measures).

```{r}
get_performance <- function(filtered.run) {
  map2_dfr(filtered.run, names(filtered.run),
                                   function(misty.run, name) {
    misty.run$improvements %>%
      mutate(algorithm = str_extract(name, "(?<=/)[^/]+$")) %>%
      mutate(study = str_extract(sample, "(?<=OUTPUT/)[^/]+")) %>%
      mutate(view = str_extract(sample, paste0("(?<=OUTPUT/", study, "/)[^/]+")))
  })
}
```

```{r}
check_runs <- function(misty.performances, ordering = NULL) {
  
    levs <- function() {
    if (is.null(ordering)) {
      out <- misty.performances %>% 
        filter(!(algorithm %in% exclude)) %>%
        pull(algorithm) %>%
        unique()
    } else {
      out <- ordering
    }
    out
  }
  
  misty.performances %>%
    select(algorithm, sample, view) %>%
    distinct() %>%
    mutate(algorithm = factor(algorithm, levels = levs())) %>%
    group_by(view, algorithm) %>%
    summarise(sample_number = n(), .groups="drop") %>%
    pivot_wider(names_from = "view", values_from = "sample_number")
}
```

```{r}
plot_performance <- function(misty.performances, str.measure, 
                             str.summary = "mean", targets = FALSE,
                             exclude = c(), wrap.views = FALSE,
                             ncol = 2, ordering = NULL) {
  
  str.summary = match.arg(str.summary, c("mean", "median"))
  
  fill = ifelse(str.summary == "mean", "mean", "median")
  
  theme_custom <- function() {
    if (targets) {
      out <- theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5))
    } else {
      out <- theme(axis.text.y = element_blank(), axis.ticks.y=element_blank(),
                   axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5))
    }
    out
  }
  
  wrap <- function() {
    if (wrap.views) {
      out <- facet_wrap(~ view, ncol = ncol)
    } else {
      out <- NULL
    }
    out
  }
  
  levs <- function() {
    if (is.null(ordering)) {
      out <- misty.performances %>% 
        filter(!(algorithm %in% exclude)) %>%
        pull(algorithm) %>%
        unique()
    } else {
      out <- ordering
    }
    out
  }
  
  plot <- misty.performances %>%
    filter(measure == str.measure) %>%
    filter(!(algorithm %in% exclude)) %>%
    mutate(algorithm = factor(algorithm, levels=levs())) %>%
    group_by(view, algorithm, target) %>%
    summarise(mean = mean(value), median = median(value), 
              sd = sd(value), .groups = "drop_last") %>%
    ggplot() +
    geom_tile(aes(x = algorithm, y = target, 
                  # use .data[[var]] to access data variables with strings
                  fill = .data[[fill]] )) +
    scale_fill_viridis_c(values = c(0, 0.1, 0.2, 0.4, 0.7, 1)) +
    labs(x = "Algorithm", fill = paste0(str.summary, " : ", str.measure)) +
    theme_custom() +
    wrap()
  
  print(plot)
}
```

```{r}
# ordering of factors
l = c("RF", "TBOOST", "MARS100", "MARS", "MARS80", "MARS60", "MARS40",
      "BGMARS", "LBOOST", "LM", "SVM", "MLP")
```


## `synthetic`

```{r}
filter_runs(all.results, "synthetic", c("l6", "l12", "l24")) %>%
  get_performance() %>%
  check_runs(ordering = l)
```


```{r fig.width=8, fig.height=4}
filter_runs(all.results, "synthetic", c("l6", "l12", "l24")) %>%
  get_performance() %>%
  mutate(view = factor(view, levels = c("l6", "l12", "l24"))) %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = TRUE, wrap.views = TRUE, ncol=3,
                   ordering = l) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = TRUE, wrap.views = TRUE, ncol=3,
                   ordering = l) %>%
  invisible()
```

## `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_performance() %>%
  check_runs(ordering = l)
```

```{r fig.width=8}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = TRUE, ordering = l) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = TRUE, ordering = l) %>%
  invisible()
```

---

## `merfish_preoptic`

We see that MARS (with the current configuration) is not scalable enough to
handle such large datasets with 100 markers and up to 70,000 spatial units.

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_performance() %>%
  check_runs(ordering = l)
```

```{r fig.width=8}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = FALSE, ordering = l,
                   exclude = c("MARS", "MARS40", "MARS60", "MARS80")) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, ordering = l,
                   exclude = c("MARS", "MARS40", "MARS60", "MARS80")) %>%
  invisible()
```

---

## `merfish_liver`

For some reason everything but bagged MARS, MARS, and gradient boosted trees 
failed.

```{r}
filter_runs(all.results, "merfish_liver", "std280") %>%
  get_performance() %>%
  check_runs(ordering = l)
```

```{r fig.width=8}
filter_runs(all.results, "merfish_liver", "std280") %>%
  get_performance() %T>%
  plot_performance(str.measure = "mulit.R2", str.summary = "mean",
                   targets = FALSE, ordering = l) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, ordering = l) %>%
  invisible()
```

---

## `merfish_bc`

Since the bagged MARS algorithm could only finish the computation for a single
sample I must exclude the result. One should also keep in mind that the 
MARS algorithm was not able to finish (2 samples are missing).

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_performance() %>%
  check_runs(ordering = l)
```

```{r fig.width=8}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = FALSE, exclude = c("BGMARS"),
                   ordering = l) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, exclude = c("BGMARS"),
                   ordering = l) %>%
  invisible()
```

---

## `4iANCA`

The computation for the smallest binning (`ratio_9`) could not be finished.

Also why do we only have 10 samples instead of 36 samples for ratio 153?

```{r}
filter_runs(all.results, "4iANCA", c("ratio_153", "ratio_76", 
                                     "ratio_38", "ratio_19", "ratio_9")) %>%
  get_performance() %>%
  check_runs(ordering = l)
```

```{r fig.width=10, fig.height=6}
views <- c("ratio_153", "ratio_76", "ratio_38", "ratio_19", "ratio_9")
filter_runs(all.results, "4iANCA", views) %>%
  get_performance() %>%
  mutate(view = factor(view, levels=views)) %T>%
  plot_performance(str.measure = "multi.R2", wrap.views = TRUE, ncol=3,
                   ordering = l) %>%
  plot_performance(str.measure = "gain.R2", wrap.views = TRUE, ncol=3,
                   ordering = l)
```

---

# Comparing Importances {.tabset}

Function to extract the importances (interaction measures).

```{r eval=FALSE, include=FALSE}
get_importance_dt <- function(filtered.run) {
  # https://atrebas.github.io/post/2019-03-03-datatable-dplyr/
  # could potentially be faster with these large datasets
  # impressive benchmark: https://h2oai.github.io/db-benchmark/
  map2_dfr(filtered.run, names(filtered.run), function(misty.run, name) {
    out <- misty.run$importances    
    out <- data.table::as.data.table(out)
    out[, algorithm := str_extract(name, "(?<=/)[^/]+$")]
    out[, study := str_extract(sample, "(?<=OUTPUT/)[^/]+")]
    out[, view.comp := str_extract(sample, paste0("(?<=OUTPUT/", 
                                                    study, "/)[^/]+"))]
  })
}

get_importance_old <- function(filtered.run) {
  map2_dfr(filtered.run, names(filtered.run), function(misty.run, name) {
    misty.run$importances %>%
      mutate(algorithm = str_extract(name, "(?<=/)[^/]+$")) %>%
      mutate(study = str_extract(sample, "(?<=OUTPUT/)[^/]+")) %>%
      mutate(view.comp = str_extract(sample, paste0("(?<=OUTPUT/", 
                                                    study, "/)[^/]+")))
  })
}
```

```{r}
get_importance <- function(filtered.run) {
  map2_dfr(filtered.run, names(filtered.run), function(misty.run, name) {
                                     
    # using str_extract_all drastically improves the performance
    # warning: indices of algorithm, study, and view.comp hardcoded
    all.info <- str_extract_all(misty.run$importances$sample, "(?<=/)[^/]+")
                                     
    misty.run$importances %>%
      mutate(algorithm = map_chr(all.info, ~ .x[length(.x)-1])) %>%
      mutate(study = map_chr(all.info, ~ .x[length(.x)-3])) %>%
      mutate(view.comp = map_chr(all.info, ~ .x[length(.x)-2]))
  })
}
```


Function to check whether all importances are present for each algorithm

```{r}
check_algs <- function(misty.performances) {
  misty.performances %>%
    unite(relation, Predictor, Target, sep="_") %>%
    select(-c(study, view.comp)) %>%
    drop_na() %>%
    pivot_wider(names_from=relation, values_from=Importance) %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    pivot_longer(cols = !c(sample, view, algorithm)) %>%
    mutate(check.NA = is.na(value)) %>%
    group_by(algorithm) %>%
    summarise(n.NAs = sum(check.NA), n = n())
}
```

Function to split the importances according to view and sample.

```{r}
split_importances <- function(misty.importances) {
  misty.importances %>%
    unite(relation, Predictor, Target, sep="_") %>%
    select(-c(study, view.comp)) %>%
    drop %>%
    pivot_wider(names_from=relation, values_from=Importance) %>%
    # !!! all NAs replaced with 0 !!!
    replace(is.na(.), 0) %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    group_by(sample, view) %>%
    group_split()
}
```

Function to get the keys for splitting the importances according to view and sample.

```{r}
split_keys <- function(misty.importances) {
  misty.importances %>%
    unite(relation, Predictor, Target, sep="_") %>%
    select(-c(study, view.comp)) %>%
    drop_na() %>%
    pivot_wider(names_from=relation, values_from=Importance) %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    group_by(sample, view) %>%
    group_keys()
}
```

Function to compute the cosine similarity.

```{r}
cosine_similarity <- function(keys, splits, relative="RF") {
  
  # 1. map over each view
  map_dfr(unique(keys$view), function(sel.view) {
      # 2. get the tibbles corresponding to the selected view
    keys %>%
      mutate(id = row_number()) %>%
      filter(view == sel.view) %>%
      pull(id, name = sample) %>%
      # 3. map over each sample (tibble) and bind the results as rows
      map_dfr(function(tbl.id) {
        
        tbl <- splits[[tbl.id]]
        
        # 4. get the random forest importance vector as reference
        rf <- tbl %>% filter(algorithm == relative) %>% 
          select(-c(sample, view, algorithm)) %>%
          unlist()
        
        # 5. get the importance vectors for the other algorithms by mapping over rows
        cosine.sims <- slide_dfr(tbl, function(row) {
          vec <- row %>% select(-c(sample, view, algorithm)) %>%
            unlist()
          
          # 6. for each importance vector compute the cosine similarity
          cosine.sim <- (rf %*% vec * 1/(rf %*% rf)**0.5 * 1/(vec %*% vec)**0.5) %>%
            as.double()
          
          tibble(algorithm = row %>% pull(algorithm), cosine = cosine.sim, 
                 sample = row %>% pull(sample), view = sel.view)
        })
      }) %>%
      # 7. Group by algorithm and get the mean of the cosine similarity for each sample
      group_by(algorithm, view) %>%
      summarise(mean = mean(cosine), sd = sd(cosine), .groups="drop")
    }) %>%
      arrange(algorithm)
}
```

```{r}
plot_cosine_similarity <- function(cosine_similarity) {
  cosine_similarity %>%
    mutate(low = mean-sd, high = mean+sd) %>%
    mutate(algorithm = factor(algorithm, levels=l)) %>%
    ggplot() +
    geom_point(aes(x = algorithm, y = mean)) +
    geom_errorbar(aes(x = algorithm, ymin = low, ymax = high)) +
    facet_wrap(~ view) +
    theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5))
}
```


## `synthetic`

For the boosting functions several importances are not returned?

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  check_algs()
```

### Cosine Similarity

Split the importances according to view and sample and get the corresponding keys.

```{r}
splits <- filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  split_importances()

keys <- filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  split_keys()
```

Now let's look at the cosine similarities.

```{r}
cosine_similarity(keys, splits)
```

And plot them.

```{r}
cosine_similarity(keys, splits) %>%
  plot_cosine_similarity()
```


## `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  check_algs()
```

### Cosine Similarity

```{r}
splits <- filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  split_importances()

keys <- filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  split_keys()
```

```{r}
cosine_similarity(keys, splits) %>%
  plot_cosine_similarity()
```


## `merfish_preoptic`

## `merfish_liver`

## `merfish_bc`

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  check_algs()
```

```{r}
splits <- filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  split_importances()

keys <- filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  split_keys()
```

#### Intraview

```{r}
cosine_similarity(keys, splits) %>%
  plot_cosine_similarity()
```

## `4iANCA`

# Comparing Runtimes

# Hyperparameter Optimization {.tabset}

## RF

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "RF_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "RF_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

## MARS

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

### `synthetic`

```{r}
filter_runs(all.results, "synthetic", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

## Linear Boosting

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

## Tree Boosting

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "TBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "TBOOST_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

# References
