---
title: "Benchmark"
output:
  workflowr::wflow_html:
    toc: yes
    toc_float: yes
    toc_depth: 2
    code_folding: hide
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
csl: science.csl
---

Setup.

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = FALSE)
knitr::opts_knit$set(root.dir = "~/Saez/report")
```

```{r}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(slider))
```

```{r}
# default ordering of the algorithms
def.ord <-  c("RF", "TBOOST", "MARS100", "MARS", "MARS80", "MARS60", "MARS40",
              "BGMARS", "LBOOST", "LM", "SVM", "MLP")
```

# Introduction

This document is split into 5 main sections:

1. [Data]

 * The different datasets used to benchmark the different algorithms are introduced, including how and which MISTy views were computed.

2. [Comparing Performances]

 * Mulit R2 (proportion of variance explained) and Gain in R2 are compared between the different algorithms of each dataset.

3. [Comparing Importances]

 * Cosine Similarity of the importance vectors are compared between the different algorithms of each dataset. Additionally the jacard index after binarization of the importantes is used for comparison.

4. [Comparing Runtimes]

 * The runtimes for the different algorithms on the synthetic datasets are compared.

5. [Hyperparameter Optimization]

 * For each algorithm different sets of hyperparameters were tested on different datasets and the resulting performance are compared here.

---

# Data {.tabset}

Loading the data.

```{r}
all.files <-  list.files("output", full.names = TRUE)
res.files <- all.files[str_ends(all.files, "results.RDS")]
view.files <- all.files[str_ends(all.files, "views.RDS")]

# Getting the latest results and views
latest.result <- str_extract(res.files, "[0-9-]+") %>% 
  as.POSIXct() %>% 
  which.max()
latest.views <- str_extract(view.files, "[0-9-]+") %>% 
  as.POSIXct() %>% 
  which.max()

print(paste("Loading results from:", res.files[latest.result]))
all.results <- readRDS(res.files[latest.result])

print(paste("Loading views from:", view.files[latest.views]))
all.views <- readRDS(view.files[latest.views])
rm(all.files, res.files, view.files)
```

What data / experiments is the benchmarking based on?

```{r}
str_extract(names(all.results), "[^/]+(?=/)") %>% unique
```


```{r echo=FALSE}
# removing irrelevant data
views_to_remove <- c("/4iANCA/ratio_6_views", "/4iANCA/ratio_12_views",
                     "/4iANCA/ratio_24_views")
all.views <- all.views[(!names(all.views) %in% views_to_remove)]
```

## `synthetic`

-   Simulated data that are included in the MISTy package. As explained in the [Get Started Vignette](https://saezlab.github.io/mistyR/articles/mistyR.html),this dataset is based on a two-dimensional cellular automata model which models the production, diffusion, degradation and interactions of 11 molecular species in 4 different cell types.

-   Data Availability: `mistyR` package `data("synthetic")`

-   Features: Simulated Data

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/synthetic/l6`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/synthetic/l6`[[1]]$intraview$data)
```

-   Different views that were computed:

    +   `/synthetic/l6`: paraview computed with `l=6`

    +   `/synthetic/l12`: paraview computed with `l=12`

    +   `/synthetic/l24`: paraview computed with `l=24`

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/synthetic")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/synthetic/l6`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `mibi_tnbc`

-   Multiplexed Ion Beam Imaging (spatial proteomics) dataset from [A Structured Tumor-Immune Microenvironment in Triple Negative Breast Cancer Revealed by Multiplexed Ion Beam Imaging](https://dx.doi.org/10.1016/j.cell.2018.08.039) by Keren, L. et al. (Cell 174, 1373-1387.e19 (2018)) [@10.1016/j.cell.2018.08.039]

-   [Exploratory Data Analysis](https://schae211.github.io/report/eda_mibi_tnbc.html)

-   Data Availability: <https://mibi-share.ionpath.com/>

-   Features: Spatial Proteomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/mibi_tnbc/standard_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/mibi_tnbc/standard_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    -   `standard_views`: markers with zero variance were removed from each sample, juxtaview was computed with `neighbor.thr = 40`, and paraview was computed with `l = 120, zoi = 40`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/mibi_tnbc")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/mibi_tnbc/standard_views`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_preoptic`

-  MERFISH data from the murine hypothalamic preoptic region from [Molecular, spatial, and functional single-cell profiling of the hypothalamic preoptic region](https://dx.doi.org/10.1126/science.aau5324) by Moffitt, R. et al. (Science 362, 6416 (2018))

-   [Exploratory Data Analysis](https://schae211.github.io/report/eda_merfish_preoptic.html)

-   Data Availability: [Spatial DB](http://www.spatialomics.org/SpatialDB/merfish_30385464_browse.php)

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_preoptic/hvg_views`)
```

-   Number of markers in each sample (depending on the view, see explanation below):

```{r echo=FALSE}
c(ncol(all.views$`/merfish_preoptic/hvg_views`[[1]]$intraview$data),
  ncol(all.views$`/merfish_preoptic/standard_views`[[1]]$intraview$data))
```

-   Different views that were computed:

    +   `standard_views`: markers with zero variance were removed from each sample, the juxtaview was computed with `neighbor.thr = 30`, and the paraview with `l = 120, zoi = 30`.

    +   `hvg_views`: markers with zero variance were removed from each sample, the 108 most spatially variable genes (computed with *SPARK-X* [@10.1186/s13059-021-02404-0]) were selected, the juxtaview was computed with `neighbor.thr = 30`, and the paraview with `l = 120, zoi = 30`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_preoptic")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_preoptic/hvg_views`, 
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_liver`

-   MERFISH data from the murine fetal liver from [Spatial transcriptome profiling by MERFISH reveals fetal liver hematopoietic stem cell niche architecture](https://dx.doi.org/10.1038/s41421-021-00266-1) by Yanfang, L. et al. (Cell Discovery 7, 47 (2021))

-   [Exploratory Data Analysis](https://schae211.github.io/report/eda_merfish_liver.html)

-   Data Availability: [SIYUAN WANG LAB](https://campuspress.yale.edu/wanglab/HSCMERFISH/)

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_liver/standard280_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/merfish_liver/standard280_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    +   `standard280_views`: markers with zero variance were removed from each sample, samples with fewer than 280 cells were removed, the juxtaview was computed with `neighbor.thr = 100`, and the paraview with `l = 180, zoi = 100`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_liver")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_liver/standard280_views`,
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `merfish_bc`

-   MERFISH data from metastatic breast cancer (unpublished)

-   Data Availability: -

-   Features: Spatial Transcriptomics, Single Cell Resolution, Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/merfish_bc/hvg_views`)
```

-   Number of markers in each sample (depending on the view, see explanation below):

```{r echo=FALSE}
c(ncol(all.views$`/merfish_bc/hvg_views`[[1]]$intraview$data),
  ncol(all.views$`/merfish_bc/standard_views`[[1]]$intraview$data))
```

-   Different views that were computed:

    +   `standard_views`: markers with zero variance were removed from each sample, the juxtaview was computed with `neighbor.thr = 15`, and the paraview with `l = 100, zoi = 15`.

    +   `hvg_views`: markers with zero variance were removed from each sample, the 86 most spatially variable genes (computed with *SPARK-X* [@10.1186/s13059-021-02404-0]) were selected, the juxtaview was computed with `neighbor.thr = 15`, and the paraview with `l = 100, zoi = 15`.

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/merfish_bc")]
```

-   Number of spatial units (e.g. cells) per sample:

```{r echo=FALSE}
summary(
  map_dbl(all.views$`/merfish_bc/hvg_views`,
          ~ nrow(.x$intraview$data))
)
```

------------------------------------------------------------------------

## `4iANCA`

-   Iiterative Indirect Immunofluorescence Imaging (4i) data from glomeruli (unpublished)

-   Data Availability: -

-   Features: Spatial Proteomics, Single Cell Resolution, No Segmentation

-   Number of samples:

```{r echo=FALSE}
length(all.views$`/4iANCA/ratio_153_views`)
```

-   Number of markers in each sample:

```{r echo=FALSE}
ncol(all.views$`/4iANCA/ratio_153_views`[[1]]$intraview$data)
```

-   Different views that were computed:

    + `/4iANCA/ratio_153_views`

    + `/4iANCA/ratio_76_views`

    + `/4iANCA/ratio_38_views`

    + `/4iANCA/ratio_19_views`

    + `/4iANCA/ratio_9\_views`

```{r echo=FALSE}
names(all.views)[str_starts(names(all.views), "/4iANCA")]
```

-   Number of spatial units (e.g. cells) per sample, depending on the computed view:

    -   Number of cells per sample: No segmentation was performed, therefore the pixels were simply binned with 5 different schemes. The pixels have a side length of $0.13$ microns, and an average eukaryotic cell is between $10$ and $100 \micro m$ in diameter. Therefore I choose the largest binning to be `floor(20/0.13) = 153`, meaning the side length of one spatial unit (bin) will correspond to 153 pixels. The smaller bins are then sequentially half of the size (76, 38, 19, 9). The corresponding parameter for the computation of the paraview was adjusted accordingly, meaning if the l parameter $l_{153}$ was used for computation of the paraview for the 153 binning, $2 \times l_{153}$ was used for the 76 binning.

```{r echo=FALSE}
map2_dfr(all.views[str_starts(names(all.views), "/4iANCA")], 
        names(all.views)[str_starts(names(all.views), "/4iANCA")],
        function(view, n) {
    map_dfr(view, ~ nrow(.x$intraview$data)) %>%
    mutate(experiment = n)
}) %>%
  pivot_longer(cols = !c(experiment), 
               names_to = "sample", 
               values_to = "n") %>%
  group_by(experiment) %>%
  dplyr::summarise(mean_n = mean(n)) %>%
  arrange(mean_n)
```

------------------------------------------------------------------------

# Scripts

The scripts that were used for all analysis can be found in this [GitHub repository](https://github.com/schae211/report/tree/master/cluster_scripts). The computations were performed on a cluster managed by Slurm.

------------------------------------------------------------------------

# Algorithm Abbreviations

* RF: Random Forest

* TBOOST: Gradient Boosting with Trees

* LBOOST: Gradient Boosting with Linear Models

* MARS: Multivariate Adaptive Regression Splines

* BGMARS: Bagged Multivariate Adaptive Regression Splines Models

* LM: Linear Model

* SVM: Linear Support Vector Machine

* MLP: Multi-Layer Perceptron

-> See details in the [method](https://schae211.github.io/report/tutorial.html) section.

------------------------------------------------------------------------

# Comparing Performances {.tabset}

Define a function to get clean data for a particular study.

```{r}
# helper function to get all misty runs for a particular study
filter_runs <- function(all.runs, str.study, views) {
  str.view <- paste(views, collapse="|")
  all.names <- names(all.runs)
  names.study <- all.names[str_detect(all.names, str.study)]
  names.view <- names.study[str_detect(names.study, str.view)]
  sucessful.runs <- map_lgl(all.results[names.view], ~ typeof(.x) == "list")
  all.results[names.view][sucessful.runs]
}
```

Define a function to extract the improvements (performance measures) from
a list of misty results.

```{r}
get_performance <- function(filtered.run) {
  map2_dfr(filtered.run, names(filtered.run),
                                   function(misty.run, name) {
    misty.run$improvements %>%
      mutate(algorithm = str_extract(name, "(?<=/)[^/]+$")) %>%
      mutate(study = str_extract(sample, "(?<=OUTPUT/)[^/]+")) %>%
      mutate(view = str_extract(sample, paste0("(?<=OUTPUT/", study, "/)[^/]+")))
  })
}
```

Define a function to check whether something is missing in misty performance
files.

```{r}
check_runs <- function(misty.performances, ordering = NULL, exclude = c()) {
  
    levs <- function() {
    if (is.null(ordering)) {
      out <- misty.performances %>% 
        filter(!(algorithm %in% exclude)) %>%
        pull(algorithm) %>%
        unique()
    } else {
      out <- ordering
    }
    out
  }
  
  misty.performances %>%
    distinct() %>%
    mutate(algorithm = factor(algorithm, levels = levs())) %>%
    select(view, algorithm, sample) %>%
    distinct() %>%
    group_by(view, algorithm) %>%
    dplyr::summarise(sample_number = n(), .groups="drop") %>%
    pivot_wider(names_from = "view", values_from = "sample_number")
}
```

Define a function to plot misty performances for all targets and for all 
considered algorithms (and for all views that were included).

```{r}
plot_performance <- function(misty.performances, str.measure, 
                             str.summary = "mean", targets = FALSE,
                             exclude = c(), wrap.views = FALSE,
                             ncol = 2, ordering = NULL) {
  
  str.summary = match.arg(str.summary, c("mean", "median"))
  
  fill = ifelse(str.summary == "mean", "mean", "median")
  
  theme_custom <- function() {
    if (targets) {
      out <- theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5))
    } else {
      out <- theme(axis.text.y = element_blank(), axis.ticks.y=element_blank(),
                   axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5))
    }
    out
  }
  
  wrap <- function() {
    if (wrap.views) {
      out <- facet_wrap(~ view, ncol = ncol)
    } else {
      out <- NULL
    }
    out
  }
  
  levs <- function() {
    if (is.null(ordering)) {
      out <- misty.performances %>% 
        filter(!(algorithm %in% exclude)) %>%
        pull(algorithm) %>%
        unique()
    } else {
      out <- ordering
    }
    out
  }
  
  plot <- misty.performances %>%
    filter(measure == str.measure) %>%
    filter(!(algorithm %in% exclude)) %>%
    mutate(algorithm = factor(algorithm, levels=levs())) %>%
    group_by(view, algorithm, target) %>%
    dplyr::summarise(mean = mean(value), median = median(value), 
              sd = sd(value), .groups = "drop_last") %>%
    ggplot() +
    geom_tile(aes(x = algorithm, y = target, 
                  # use .data[[var]] to access data variables with strings
                  fill = .data[[fill]] )) +
    scale_fill_viridis_c(values = c(0, 0.1, 0.2, 0.4, 0.7, 1)) +
    labs(x = "Algorithm", fill = paste0(str.summary, " : ", str.measure)) +
    theme_custom() +
    wrap()
  
  print(plot)
}
```

## `synthetic`

```{r}
filter_runs(all.results, "synthetic", c("l6", "l12", "l24")) %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

Looking at the multi R2, one can clearly see that the RF, TBOOST, MARS, and bagged MARS perform all very well, whereas LBOOST, LM, SVM, and MLP are all worse with LBOOST (with default parameters) being the worst of all models tested here.

Interestingly, the gain in R2 seems to decrease a little bit if `l` is increased from 6 to 12 to 24. For all models but LBOOST, only ligC and ligB see a rather large incrase in gain in R2.

```{r fig.width=8, fig.height=4}
filter_runs(all.results, "synthetic", c("l6", "l12", "l24")) %>%
  get_performance() %>%
  mutate(view = factor(view, levels = c("l6", "l12", "l24"))) %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = TRUE, wrap.views = TRUE, ncol=3,
                   ordering = def.ord) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = TRUE, wrap.views = TRUE, ncol=3,
                   ordering = def.ord) %>%
  invisible()
```

## `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

The MLP cleary performs worst followed by the SVM. Also LBOOST, MARS, and LM are worst than RF and TBOOST.

The gain in R2 is by far the lowest for LBOOST.

```{r fig.width=8}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = TRUE, ordering = def.ord) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = TRUE, ordering = def.ord) %>%
  invisible()
```

---

## `merfish_preoptic`

We see that MARS (with the current configuration) is not scalable enough to
handle such large datasets with 100 markers and up to 70,000 spatial units. Even if an approximation of 60% is used, only 4 of 36 samples could be processed.

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

The results show again that RF and TBOOST perform very similar, whereas LM is a little bit worse and LBOOST being much worse.

```{r fig.width=8}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord,
                   exclude = c("MARS", "MARS40", "MARS60", "MARS80")) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord,
                   exclude = c("MARS", "MARS40", "MARS60", "MARS80")) %>%
  invisible()
```

---

## `merfish_liver`

Everything but RF, TBOOST, and LM will be excluded from the analysis. (where are the MARS results?). In general this datasets is a rather uncommon one, since it comprises many field of views per sample, but each field of view only has between 200 and 360 cells approximately. Such a small field of view makes it difficult for the algorithms to learn something.

```{r}
filter_runs(all.results, "merfish_liver", "std280") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

It seems like the number of cells in each field of view is much to small for the flexible models such as RF and TBOOST. The LM outperforms these models by a large margin.

```{r fig.width=8}
filter_runs(all.results, "merfish_liver", "std280") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord,
                   exclude = c("MLP", "SVM")) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, ordering = def.ord,
                   exclude = c("MLP", "SVM")) %>%
  invisible()
```

---

## `merfish_bc`

Since the bagged MARS algorithm could only finish the computation for two samples I wll exclude the result. The results from SVM and MLP will be removed as well. One should also keep in mind that the MARS algorithm was not able to finish (1 samples are missing).

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

The MISTy results for this dataset are remarkable in the sense that all algorithms apart from MARS perform equally well, meaning that the LM performs as well as the RF. This is interesting since the LM is much more interpretable and faster to compute.

```{r fig.width=8}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_performance() %T>%
  plot_performance(str.measure = "multi.R2", str.summary = "mean",
                   targets = FALSE, exclude = c("BGMARS", "SVM", "MLP"),
                   ordering = def.ord) %T>%
  plot_performance(str.measure = "gain.R2", str.summary = "mean",
                   targets = FALSE, exclude = c("BGMARS", "SVM", "MLP"),
                   ordering = def.ord) %>%
  invisible()
```

---

## `4iANCA`

The computation for the smallest binning (`ratio_9`) could not be finished.

Also why do we only have 10 samples instead of 36 samples for ratio 153?

```{r}
filter_runs(all.results, "4iANCA", c("ratio_153", "ratio_76", 
                                     "ratio_38", "ratio_19", "ratio_9")) %>%
  get_performance() %>%
  check_runs(ordering = def.ord)
```

```{r fig.width=10, fig.height=6}
filter_runs(all.results, "4iANCA", c("ratio_153", "ratio_76", "ratio_38",
                                     "ratio_19", "ratio_9")) %>%
  get_performance() %>%
  mutate(view = factor(view, levels=c("ratio_153", "ratio_76", "ratio_38",
                                     "ratio_19", "ratio_9"))) %T>%
  plot_performance(str.measure = "multi.R2", wrap.views = TRUE, ncol=3,
                   ordering = def.ord) %>%
  plot_performance(str.measure = "gain.R2", wrap.views = TRUE, ncol=3,
                   ordering = def.ord)
```

---

# Comparing Importances {.tabset}

Define function to extract the importances (interaction measures).

```{r}
get_importance <- function(filtered.run) {
  map2_dfr(filtered.run, names(filtered.run), function(misty.run, name) {
                                     
    # using str_extract_all drastically improves the performance
    # warning: indices of algorithm, study, and view.comp hardcoded
    all.info <- str_extract_all(misty.run$importances$sample, "(?<=/)[^/]+")
                                     
    misty.run$importances %>%
      mutate(algorithm = map_chr(all.info, ~ .x[length(.x)-1])) %>%
      mutate(study = map_chr(all.info, ~ .x[length(.x)-3])) %>%
      mutate(view.comp = map_chr(all.info, ~ .x[length(.x)-2]))
  }) %>%
    mutate(sample = str_extract(sample, "[^/]+$"))
}
```

Define function to check how many importances are available for each algorithm and how many are missing.

```{r}
check_algs <- function(misty.performances) {
  misty.performances %>%
    unite(relation, Predictor, Target, sep="_") %>%
    dplyr::select(-c(study, view.comp)) %>%
    drop_na() %>%
    pivot_wider(names_from=relation, values_from=Importance) %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    pivot_longer(cols = !c(sample, view, algorithm)) %>%
    mutate(check.NA = is.na(value)) %>%
    group_by(algorithm) %>%
    dplyr::summarise(n.NAs = sum(check.NA), n = n())
}
```

Define function to get splits according to view and sample from misty importances.

```{r}
split_importances <- function(misty.importances) {
  tidy.df <- misty.importances %>%
    unite(relation, Predictor, Target, sep="_") %>%
    drop_na() %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    pivot_wider(names_from = algorithm, values_from = Importance) %>%
    replace(is.na(.), 0) %>%
    group_by(view, sample)
  
  list(splits = group_split(tidy.df), keys = group_keys(tidy.df))
}
```

Define function to compute the cosine similarity given a tibble of vectors, 
where each column represents a vector. Indices are hardcoded

```{r}
cosine_similarity <- function(imp.tbl, exclude = c(1:5), relative = "RF") {
  comp <- imp.tbl %>% pull(relative)
  map_dbl(imp.tbl[-exclude], function(vec) {
    comp %*% vec * 1/(comp %*% comp)**0.5 * 1/(vec %*% vec)**0.5
  })
}
```

Define function to compute the jaccard index given a tibble of vectors, where
each column represents a vector. Indices are harcoded.

```{r}
jaccard_index <- function(imp.tbl, exclude = c(1:5), 
                          relative = "RF", cutoff = 0.5) {
  comp <- imp.tbl %>% filter(.data[[relative]] > cutoff) %>% pull(relation)
  algs <- names(imp.tbl)[-exclude] %>% setNames(names(imp.tbl)[-exclude])
  
  map_dbl(algs, function(alg) {
    vec <- imp.tbl %>% filter(.data[[alg]] > cutoff) %>% pull(relation)
    length(intersect(comp, vec)) / length(union(comp, vec))
  })
}
```

Define wrapper function to compute comparitive statistics (e.g. cosine
similarity, jaccard index), given a misty importances tibble.

First, the tibble containing the importances is split according to views and
samples. Second, for each sub tibble the 

```{r}
wrap_function <- function(misty.importances, f, ...) {
  
  ellipsis.args <- list(...)
  # Split the importance tibble corresponding to views and samples
  # since we want to compare the algorithm for the same sample and view
  split.list <- split_importances(misty.importances)
  
  # 1. map over each view
  map_dfr(unique(split.list$keys$view), function(sel.view) {
    
    # 2. Map over each sample corresponding to this view
    split.list$keys %>%
      mutate(id = row_number()) %>%
      filter(view == sel.view) %>%
      pull(id, name = sample) %>%
      map_dfr(function(tbl.id) {
        
        # 3. Compute cosine similarity (and add corresponding view and sample)
        if (length(ellipsis.args) > 0) {
                  args <- rlist::list.merge(
                    list(imp.tbl = split.list$splits[[tbl.id]]), 
                    ellipsis.args
                    )
        } else {
          args <- list(imp.tbl = split.list$splits[[tbl.id]])
        }
        c(do.call(f, args), "id" = tbl.id)
          
      }) %>% mutate(view = sel.view,
                    sample = split.list$keys$sample[as.integer(id)]) 
  }) %>%
    pivot_longer(-c(id, view, sample), 
                 names_to = "algorithm", values_to = "cosine") %>%
    
      # 4. Group by algorithm 
      # and get the mean of the cosine similarity for each sample
      group_by(algorithm, view) %>%
      dplyr::summarise(mean = mean(cosine), sd = sd(cosine), .groups="drop") %>%
      arrange(algorithm)
}
```

Define functions to plot the cosine similarity or the jaccard index after binarization of the importances relative to RF.

```{r}
plot_cosine_similarity <- function(cosine_similarity) {
  cosine_similarity %>%
    mutate(low = mean-sd, high = mean+sd) %>%
    mutate(algorithm = factor(algorithm, levels = def.ord)) %>%
    ggplot() +
    geom_point(aes(x = algorithm, y = mean)) +
    geom_errorbar(aes(x = algorithm, ymin = low, ymax = high)) +
    facet_wrap(~ view) +
    theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5)) +
    lims(y = c(0, 1))
}

plot_jaccard <- function(jaccard) {
  jaccard %>%
    mutate(low = mean-sd, high = mean+sd) %>%
    mutate(algorithm = factor(algorithm, levels = def.ord)) %>%
    ggplot() +
    geom_point(aes(x = algorithm, y = mean)) +
    geom_errorbar(aes(x = algorithm, ymin = low, ymax = high)) +
    facet_wrap(~ view) +
    theme(axis.text.x = element_text(angle=45, vjust=0.5, hjust=0.5)) +
    lims(y = c(0, 1))
}
```

Functions to plot heatmaps of the cosine similarity, pearson correaltion, or jaccard index after binarization.

```{r}
cosine_heatmap <- function(misty.importances) {
  tidy.df <- misty.importances %>%
    unite(relation, Predictor, Target) %>%
    drop_na() %>%
    pivot_wider(names_from = relation, values_from = Importance)
  
  views <- misty.importances$view %>% unique
  
  plots <- map(views, function(v) {
    
    tmp.df <- tidy.df %>%
      filter(view == v) %>%
      dplyr::select(-c(study, view.comp)) %>%
      mutate(sample = str_extract(sample, "[^/]+$")) %>%
      group_by(algorithm) %>%
      dplyr::summarise(across(4:(ncol(.)-1), mean)) %>%
      replace(is.na(.), 0) %>%
      pivot_longer(cols = !algorithm) %>%
      pivot_wider(names_from = algorithm, values_from = value) %>%
      dplyr::select(-name)
  
    plot <- map_dfr(colnames(tmp.df), function(alg) {
      comp <- tmp.df %>% pull(alg)
      map_dbl(tmp.df, function(vec) {
        comp %*% vec * 1/(comp %*% comp)**0.5 * 1/(vec %*% vec)**0.5
      })
    }) %>%
      as.data.frame() %>%
      replace(is.na(.), 0) %>%
      set_rownames(colnames(tmp.df)) %>%
      pheatmap::pheatmap(main = paste("Cosine Similarity:", v),
                         silent = TRUE)
    })
  
  do.call(gridExtra::grid.arrange, map(plots,~.x[[4]]))
}

jaccard_heatmap <- function(misty.importances, cutoff = 0.5) {
  tidy.df <- misty.importances %>%
    unite(relation, Predictor, Target) %>%
    drop_na() %>%
    pivot_wider(names_from = relation, values_from = Importance)
  
  views <- misty.importances$view %>% unique
  
  plots <- map(views, function(v) {
    
    tmp.df <- tidy.df %>%
      filter(view == v) %>%
      dplyr::select(-c(study, view.comp)) %>%
      mutate(sample = str_extract(sample, "[^/]+$")) %>%
      group_by(algorithm) %>%
      dplyr::summarise(across(4:(ncol(.)-1), mean)) %>%
      replace(is.na(.), 0) %>%
      pivot_longer(cols = !algorithm) %>%
      pivot_wider(names_from = algorithm, values_from = value)
    
    algs <- names(tmp.df)[-1] %>% setNames(names(tmp.df)[-1])
    
    map_dfr(algs, function(alg1) {
    comp <- tmp.df %>% 
      filter(.data[[alg1]] > cutoff) %>% 
      pull(name)
    
    map_dbl(algs, function(alg2) {
      vec <- tmp.df %>% 
      filter(.data[[alg2]] > cutoff) %>% 
      pull(name)
      length(intersect(comp, vec)) / length(union(comp, vec))
      })
    }) %>%
      as.data.frame() %>%
      replace(is.na(.), 0) %>%
      set_rownames(algs) %>%
      pheatmap::pheatmap(main = paste("Jaccard Index:", v), silent=TRUE)
  })
  do.call(gridExtra::grid.arrange, map(plots,~.x[[4]]))
}

correlation_heatmap <- function(misty.importances, m = "pearson") {
  tidy.df <- misty.importances %>%
    unite(relation, Predictor, Target) %>%
    drop_na() %>%
    pivot_wider(names_from = relation, values_from = Importance)
  
  views <- misty.importances$view %>% unique
  
  plots <- map(views, function(v) {
    
    tmp.df <- tidy.df %>%
      filter(view == v) %>%
      dplyr::select(-c(study, view.comp)) %>%
      mutate(sample = str_extract(sample, "[^/]+$")) %>%
      group_by(algorithm) %>%
      dplyr::summarise(across(4:(ncol(.)-1), mean)) %>%
      replace(is.na(.), 0) %>%
      pivot_longer(cols = !algorithm) %>%
      pivot_wider(names_from = algorithm, values_from = value) %>%
      dplyr::select(-name)
  
    plot <- map_dfr(colnames(tmp.df), function(alg) {
      comp <- tmp.df %>% pull(alg)
      map_dbl(tmp.df, function(vec) {
        stats::cor(comp, vec, method = m)
      })
    }) %>%
      as.data.frame() %>%
      replace(is.na(.), 0) %>%
      set_rownames(colnames(tmp.df)) %>%
      pheatmap::pheatmap(main = paste(str_to_title(m), "Correlation:", v),
                         silent = TRUE)
    })
  
  do.call(gridExtra::grid.arrange, map(plots,~.x[[4]]))
}
```

Define function to create a scatter plot matrix from MISTy importances.

```{r}
correlation_scatter <- function(misty.importances) {
  views <- misty.importances$view %>% unique
  
  walk(views, function(v) {
    misty.importances %>%
      unite(relation, Predictor, Target) %>%
      drop_na() %>%
      pivot_wider(names_from = relation, values_from = Importance) %>%
      filter(view == v) %>%
      dplyr::select(-c(study, view.comp)) %>%
      mutate(sample = str_extract(sample, "[^/]+$")) %>%
      group_by(algorithm) %>%
      dplyr::summarise(across(4:(ncol(.)-1), mean)) %>%
      replace(is.na(.), 0) %>%
      pivot_longer(cols = !algorithm) %>%
      pivot_wider(names_from = algorithm, values_from = value) %>%
      dplyr::select(-name) %>%
      GGally::ggpairs(title=v) %>%
      print()
      #scatterPlotMatrix::scatterPlotMatrix(corrPlotType = "Text",
      #                                     corrPlotCS = "Viridis") %>%
      #print()
  })
}
```


```{r include=FALSE, eval=FALSE}
cosine_heatmap2 <- function(misty.importances) {
  tidy.df <- misty.importances %>%
    unite(relation, Predictor, Target) %>%
    drop_na() %>%
    pivot_wider(names_from = relation, values_from = Importance) %>%
    filter(view == "intra") %>%
    dplyr::select(-c(study, view.comp)) %>%
    mutate(sample = str_extract(sample, "[^/]+$")) %>%
    group_by(algorithm) %>%
    dplyr::summarise(across(4:(ncol(.)-1), mean)) %>%
    replace(is.na(.), 0) %>%
    pivot_longer(cols = !algorithm) %>%
    pivot_wider(names_from = algorithm, values_from = value) %>%
    dplyr::select(-name)
  
  map_dfr(colnames(tidy.df), function(alg) {
    comp <- tidy.df %>% pull(alg)
    map_dbl(tidy.df, function(vec) {
      comp %*% vec * 1/(comp %*% comp)**0.5 * 1/(vec %*% vec)**0.5
    })
  }) %>%
    mutate(comp = colnames(.)) %>%
    pivot_longer(!comp, names_to = "algorithm", values_to = "measure") %>%
    mutate(algorithm = factor(algorithm, levels = def.ord)) %>%
    mutate(comp = factor(comp, levels = def.ord)) %>%
    ggplot() +
    geom_tile(aes(x = algorithm, y = comp, fill = measure)) +
    scale_fill_viridis_c() +
    scale_y_discrete(limits=rev)
}
```

## `synthetic`

For the boosting functions several importances are not returned?

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  check_algs()
```

Cosine similarity.

```{r fig.height=6, fig.width=6, warning=FALSE}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  cosine_heatmap()
```

<details> <summary>Click here for a heatmap of the Jaccard Indices</summary> 
```{r fig.height=6, fig.width=6, warning=FALSE}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  jaccard_heatmap(cutoff = 0.5)
```
</details>

<details> <summary>Click here for a Scatter Plot Matrix</summary> 
```{r fig.width=12, fig.height=12, warning=FALSE}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  correlation_scatter()
```
</details>

<details> <summary>Click here for pairwise comparison to RF</summary> 

These pairwise comparison allow to take the variance into account, where
as the heatmaps can only display a single value (mean).

Cosine Similarity compared to random forest

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  wrap_function(cosine_similarity) %>%
  plot_cosine_similarity()
```

Jaccard Index compared to radnom forest

```{r}
filter_runs(all.results, "synthetic", "l6") %>%
  get_importance() %>%
  wrap_function(jaccard_index, cutoff=0) %>%
  plot_jaccard()
```

</details>

---

## `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  check_algs()
```

```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  drop_na() %>%
  cosine_heatmap()
```

<details> <summary>Click here for a heatmap of the Jaccard Indices</summary> 
```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  drop_na() %>%
  jaccard_heatmap(cutoff = 0.5)
```
</details>

<details> <summary>Click here for a Scatter Plot Matrix</summary> 
```{r fig.width=12, fig.height=12, warning=FALSE}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  correlation_scatter()
```
</details>

<details> <summary>Click here for pairwise comparison to RF</summary> 

Cosine Similarity compared to random forest

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  wrap_function(cosine_similarity) %>%
  plot_cosine_similarity()
```

Jaccard Index compared to radnom forest

```{r}
filter_runs(all.results, "mibi_tnbc", "standard") %>%
  get_importance() %>%
  wrap_function(jaccard_index, cutoff=0) %>%
  plot_jaccard()
```

</details>

---

## `merfish_preoptic`

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  check_algs()
```

```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  cosine_heatmap()
```

<details> <summary>Click here for a heatmap of the Jaccard Indices</summary> 
```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  jaccard_heatmap(cutoff = 0.5)
```
</details>

<details> <summary>Click here for a Scatter Plot Matrix</summary> 
```{r fig.width=12, fig.height=12, warning=FALSE}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  correlation_scatter()
```
</details>

<details> <summary>Click here for pairwise comparison to RF</summary> 

Cosine Similarity compared to random forest

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  wrap_function(cosine_similarity) %>%
  plot_cosine_similarity()
```

Jaccard Index compared to random forest

```{r}
filter_runs(all.results, "merfish_preoptic", "hvg") %>%
  get_importance() %>%
  wrap_function(jaccard_index, cutoff=0) %>%
  plot_jaccard()
```

</details>

---

## `merfish_liver`

```{r eval=FALSE}
filter_runs(all.results, "merfish_liver", "std280") %>%
  get_importance() %>%
  check_algs()
```

</details>

---

## `merfish_bc`

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  check_algs()
```

```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  cosine_heatmap()
```

<details> <summary>Click here for a heatmap of the Jaccard Indices</summary> 
```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  jaccard_heatmap(cutoff = 0.5)
```
</details>

<details> <summary>Click here for a Scatter Plot Matrix</summary> 
```{r fig.width=12, fig.height=12, warning=FALSE}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  correlation_scatter()
```
</details>

<details> <summary>Click here for pairwise comparison to RF</summary> 

Cosine Similarity compared to random forest

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  wrap_function(cosine_similarity) %>%
  plot_cosine_similarity()
```

Jaccard Index compared to random forest

```{r}
filter_runs(all.results, "merfish_bc", "hvg") %>%
  get_importance() %>%
  wrap_function(jaccard_index, cutoff=0) %>%
  plot_jaccard()
```

</details>

---

## `4iANCA`

```{r}
filter_runs(all.results, "4iANCA", "ratio_76") %>%
  get_importance() %>%
  check_algs()
```

```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "4iANCA", "ratio_76") %>%
  get_importance() %>%
  filter(view != "para.10") %>%
  cosine_heatmap()
```

<details> <summary>Click here for a heatmap of the Jaccard Indices</summary> 
```{r fig.height=8, fig.width=6, warning=FALSE}
filter_runs(all.results, "4iANCA", "ratio_76") %>%
  get_importance() %>%
  filter(view != "para.10") %>%
  jaccard_heatmap(cutoff = 0.5)
```
</details>

<details> <summary>Click here for a Scatter Plot Matrix</summary> 
```{r fig.width=12, fig.height=12, warning=FALSE}
filter_runs(all.results, "4iANCA", "ratio_76") %>%
  get_importance() %>%
  filter(view != "para.10") %>%
  correlation_scatter()
```
</details>

<details> <summary>Click here for pairwise comparison to RF</summary> 

Cosine Similarity compared to random forest

```{r}
filter_runs(all.results, "4iANCA", "ratio_76") %>%
  get_importance() %>%
  filter(view != "para.10") %>%
  wrap_function(cosine_similarity) %>%
  plot_cosine_similarity()
```

Jaccard Index compared to random forest

```{r}
filter_runs(all.results, "4iANCA", "ratio_76") %>%
  get_importance() %>%
  filter(view != "para.10") %>%
  wrap_function(jaccard_index, cutoff=0) %>%
  plot_jaccard()
```

</details>

---

# Comparing Runtimes

Small timing benchmark based on the whole `synthetic` dataset, using the view
composition: intraview and paraview with `l = 12` and executing the code
with 12 workers.

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
plan("multisession", workers=12)

c("RF" = random_forest_model, "TBOOST" = gradient_boosting_model, 
  "MARS" = mars_model, "BGMARS" = bagged_mars_model,
  "LM" = linear_model, "SVM" = svm_model, "MLP" = mlp_model) %>%
  imap_dfr(function(fun, fun.name) {
    start <- Sys.time()
    misty.results.path <- 
      imap_chr(all.views$`/synthetic/l12`, function(misty.views, name) {
        misty.views %>%
          run_misty(model.function = fun, results.folder = paste0("results/", name))
    })
    end <- Sys.time()
    unlink("results", recursive = TRUE)
    tibble::tibble(model.function = fun.name, time = (end - start))
  }) %>%
  cbind("relative_to_RF" = as.double(.$time) / 
          as.double(rep(.$time[.$model.function=="RF"], length(.$model.function)))) %>%
  arrange(time)
```

As expected the linear model is by far the fastest model. Random forest
and gradient boosting with trees are equally fast and perform much better than
the multi-layer perceptron, bagged mars models, and the support vector machine.


# Hyperparameter Optimization {.tabset}

## RF

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "RF_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "RF_hyper_")) %>%
  dplyr::select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("n.tree", "min.node.size", "max.depth", "splitrule"), "_") %>%
  dplyr::select(name, everything())
```


```{r}
filter_runs(all.results, "mibi_tnbc", "RF_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "RF_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

We can clearly see that `max.depths = 1` reduces the performance significantly. Apart from that the model seems very robust to hyperparamter changes which is anothter reason why random forests are so popular.

## MARS

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  dplyr::select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("penalty", "degree", "fast.k", "pmethod"), "_") %>%
  dplyr::select(name, everything())
```

```{r}
filter_runs(all.results, "mibi_tnbc", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

Interestingly the performance seems to be pretty much invariant to the choice
of hyperparameters here.

### `synthetic`

```{r}
filter_runs(all.results, "synthetic", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  dplyr::select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("penalty", "degree", "fast.k", "pmethod"), "_") %>%
  dplyr::select(name, everything())
```

```{r}
filter_runs(all.results, "synthetic", "MARS_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "MARS_hyper_")) %>%
  plot_performance(str.measure="multi.R2", targets = TRUE)
```

For the synthetic datasets we clearly see that a higher degree increases the
model performance.

### Runtimes

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
plan("multisession", workers=12)

# Penalties: -1, 0, 2, 3
penalties <- c(-1, 0, 2, -1, 0, 2, 3, 4, 3, 3, 3, 3)

# Degrees: 1, 2, 3
degrees <- c(1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2)

# Fast K: 20, 10, 5
fastks <- c(20, 20, 20, 20, 20, 20, 20, 20, 10, 5, 20, 20)

# Prunning Method: "backward", "none", "forward"
pmethods <- c("backward", "backward", "backward", "backward", "backward", 
              "backward", "backward", "backward", "backward", "backward", 
              "none", "forward")

pmap_dfr(list(penalties, degrees, fastks, pmethods),
         function(penalty, degree, fast.k, pmethod) {
           start <- Sys.time()
           
           walk(all.views$`/synthetic/l12`, function(view) {
             run_misty(views = view, model.function = mars_model,
                       degree = degree, penalty = penalty, 
                       fast.k = fast.k, pmethod = pmethod)
           })
           
           end <- Sys.time()
           unlink("results")
           tibble(name = paste(penalty, degree, fast.k, pmethod, sep="_"),
                  penalty = penalty, degree = degree, fast.k = fast.k, pmethod = pmethod,
                  time = end - start)
         }) %>%
  arrange(time)
```

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
plan("multisession", workers=12)

# Degrees: 1, 2, 3
degrees <- c(1, 1, 1, 2, 2, 
             2, 2, 2, 2, 2, 
             2, 2)

# Fast K: 20, 10, 5
fastks <- c(20, 10, 5, 20, 10, 
            5, 20, 10, 5, 20, 
            10, 5)

# Thresholds
threshs <- c(0.001, 0.001, 0.001, 0.001, 0.001,
             0.001, 0.05, 0.05, 0.05, 0.01,
             0.01, 0.01)

pmap_dfr(list(degrees, fastks, threshs),
         function(degree, fast.k, thr) {
           start <- Sys.time()
           
           walk(all.views$`/synthetic/l12`, function(view) {
             run_misty(views = view, model.function = mars_model,
                       degree = degree, fast.k = fast.k, 
                       thresh = thr)
           })
           
           end <- Sys.time()
           unlink("results")
           tibble(name = paste(degree, fast.k, thr, sep="_"),
                  degree = degree, fast.k = fast.k, thresh = thr,
                  time = end - start)
         }) %>%
  arrange(time)
```

## Linear Boosting

```{r}
filter_runs(all.results, "mibi_tnbc", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  filter(str_count(algorithm, "_") == 2) %>%
  dplyr::select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("nrounds", "lambda", "alpha"), "_") %>%
  dplyr::select(name, everything())
```

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  filter(str_count(algorithm, "_") == 2) %>%
  plot_performance(str.measure="multi.R2")
```

No penalty leads to the best results and more boosting rounds 
increases the performance here. In general more boosting iterations are needed for linear gradient boosting compared to tree gradient boosting.

### `merfish_bc`

```{r eval=FALSE}
# TODO
filter_runs(all.results, "merfish_bc", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  filter(str_count(algorithm, "_") == 2) %>%
  dplyr::select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("nrounds", "lambda", "alpha"), "_") %>%
  dplyr::select(name, everything())
```

```{r eval=FALSE}
# TODO
filter_runs(all.results, "merfish_bc", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  filter(str_count(algorithm, "_") == 2) %>%
  plot_performance(str.measure="multi.R2")
```

### `synthetic`

```{r}
filter_runs(all.results, "synthetic", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  filter(str_count(algorithm, "_") == 2) %>%
  dplyr::select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("nrounds", "lambda", "alpha"), "_") %>%
  dplyr::select(name, everything())
```

```{r}
filter_runs(all.results, "synthetic", "LBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "LBOOST_hyper_")) %>%
  filter(str_count(algorithm, "_") == 2) %>%
  plot_performance(str.measure="multi.R2")
```

## Tree Boosting

```{r}
filter_runs(all.results, "mibi_tnbc", "TBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "TBOOST_hyper_")) %>%
  dplyr::select(algorithm) %>% 
  distinct() %>%
  mutate(name = algorithm) %>%
  separate(algorithm, 
           c("eta", "max_depth", "min_child_weight", "subsample"), "_") %>%
  dplyr::select(name, everything())
```

### `mibi_tnbc`

```{r}
filter_runs(all.results, "mibi_tnbc", "TBOOST_hyper") %>% 
  get_performance() %>%
  mutate(algorithm = str_remove(algorithm, "TBOOST_hyper_")) %>%
  plot_performance(str.measure="multi.R2")
```

Similarly to the random forest above, we see that `max_depth = 1` reduces the overall performance. Apart from that the model seems to be very robust to hyperparamter changes.

## MLP

### Runtimes

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
plan("multisession", workers=12)

funcs <- c("Std_Backpropagation", "BackpropBatch", "Quickprop",
           "BackpropChunk", "BackpropMomentum")

map_dfr(funcs, function(func) {
   #print(func)
   start <- Sys.time()
   
   walk(all.views$`/synthetic/l12`, function(view) {
     run_misty(views = view, model.function = mlp_model,
               learnFunc = func)
   })
   
   end <- Sys.time()
   unlink("results")
   tibble(funcs = func,
          time = end - start)
  }) %>%
  arrange(time)
```

And we see that the choice of learning algorithm does not make much of a difference here and that the default "BackpropBatch" seems to be the best choice.

# References
